{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-08T10:46:16.402794Z","iopub.status.idle":"2024-03-08T10:46:16.403695Z","shell.execute_reply.started":"2024-03-08T10:46:16.403395Z","shell.execute_reply":"2024-03-08T10:46:16.403417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FREEBIRDS CREW - GenAI 75 HARD : Day - 40 ðŸ’»","metadata":{}},{"cell_type":"markdown","source":"## Wikipedia Data Trained GPT-2 LLM\n\n![](https://img.freepik.com/premium-photo/robotic-detective-interrogating-suspect_783299-2053.jpg)","metadata":{}},{"cell_type":"code","source":"!pip install Wikipedia-API","metadata":{"execution":{"iopub.status.busy":"2024-03-08T17:12:49.501506Z","iopub.execute_input":"2024-03-08T17:12:49.501986Z","iopub.status.idle":"2024-03-08T17:13:03.644436Z","shell.execute_reply.started":"2024-03-08T17:12:49.501952Z","shell.execute_reply":"2024-03-08T17:13:03.643341Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting Wikipedia-API\n  Downloading Wikipedia_API-0.6.0-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from Wikipedia-API) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->Wikipedia-API) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->Wikipedia-API) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->Wikipedia-API) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->Wikipedia-API) (2024.2.2)\nDownloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\nInstalling collected packages: Wikipedia-API\nSuccessfully installed Wikipedia-API-0.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!!pip install -U transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wikipediaapi\n\nwiki_wiki = wikipediaapi.Wikipedia('FreeBirds Crew (freebirdscrew@gmail.com)','en')\n\ndef get_wikipedia_page(title):\n    page_py = wiki_wiki.page(title)\n    if not page_py.exists():\n        return None\n    return page_py.text\n\n# Example: Fetching the content of a Wikipedia page\npage_title = \"Python (programming language)\"\nwikipedia_data = get_wikipedia_page(page_title)\n\n# Save the data to a file\nwith open(\"wikipedia_data.txt\", \"w\", encoding=\"utf-8\") as file:\n    file.write(wikipedia_data)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T17:13:03.646905Z","iopub.execute_input":"2024-03-08T17:13:03.647307Z","iopub.status.idle":"2024-03-08T17:13:04.395035Z","shell.execute_reply.started":"2024-03-08T17:13:03.647267Z","shell.execute_reply":"2024-03-08T17:13:04.394266Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# with open(\"wikipedia_data.txt\", \"r\", encoding=\"utf-8\") as file:\n#     print(file.readlines())","metadata":{"execution":{"iopub.status.busy":"2024-03-08T17:13:55.666022Z","iopub.execute_input":"2024-03-08T17:13:55.666451Z","iopub.status.idle":"2024-03-08T17:13:55.670495Z","shell.execute_reply.started":"2024-03-08T17:13:55.666421Z","shell.execute_reply":"2024-03-08T17:13:55.669494Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\nfrom transformers import TextDataset, DataCollatorForLanguageModeling\nfrom transformers import Trainer, TrainingArguments\n\nmodel_name = \"gpt2\"  # Choose the appropriate GPT-2 model\n\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T10:57:22.682476Z","iopub.execute_input":"2024-03-08T10:57:22.682900Z","iopub.status.idle":"2024-03-08T10:57:27.190042Z","shell.execute_reply.started":"2024-03-08T10:57:22.682868Z","shell.execute_reply":"2024-03-08T10:57:27.189246Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e79fcd1287f244ddb981c425914b1c4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f52bc3c6f094455bb7b5246f0015e1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06dc0a8c448e4bb1a84ee5f735ae0b3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b7f338e48744f0dae1d26f6813bf639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d94774d0b10b46e99f651a6258760cf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd2541339e43425b8cb0be6805ac4dd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3a1d047e3a4bf69a28b04b9fd7a2cd"}},"metadata":{}}]},{"cell_type":"code","source":"# Prepare data for fine-tuning\ntrain_data = TextDataset(\n    tokenizer=tokenizer,\n    file_path=\"wikipedia_data.txt\",\n    block_size=128\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T10:57:33.731535Z","iopub.execute_input":"2024-03-08T10:57:33.732267Z","iopub.status.idle":"2024-03-08T10:57:34.011528Z","shell.execute_reply.started":"2024-03-08T10:57:33.732236Z","shell.execute_reply":"2024-03-08T10:57:34.010702Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # Set to True for masked language modeling tasks\n)\ndata_collator","metadata":{"execution":{"iopub.status.busy":"2024-03-08T10:57:52.547121Z","iopub.execute_input":"2024-03-08T10:57:52.547504Z","iopub.status.idle":"2024-03-08T10:57:52.554048Z","shell.execute_reply.started":"2024-03-08T10:57:52.547477Z","shell.execute_reply":"2024-03-08T10:57:52.552959Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DataCollatorForLanguageModeling(tokenizer=GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"},"metadata":{}}]},{"cell_type":"code","source":"# Fine-tune the model\ntraining_args = TrainingArguments(\n    output_dir=\"./fine-tuned_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    save_steps=10_000,\n    save_total_limit=2\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T10:58:06.671813Z","iopub.execute_input":"2024-03-08T10:58:06.672640Z","iopub.status.idle":"2024-03-08T10:58:07.983596Z","shell.execute_reply.started":"2024-03-08T10:58:06.672607Z","shell.execute_reply":"2024-03-08T10:58:07.982749Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T10:58:17.331179Z","iopub.execute_input":"2024-03-08T10:58:17.331914Z","iopub.status.idle":"2024-03-08T10:59:20.128031Z","shell.execute_reply.started":"2024-03-08T10:58:17.331883Z","shell.execute_reply":"2024-03-08T10:59:20.127020Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240308_105836-22owpe2g</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/simranjeet97/huggingface/runs/22owpe2g' target=\"_blank\">spring-plant-6</a></strong> to <a href='https://wandb.ai/simranjeet97/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/simranjeet97/huggingface' target=\"_blank\">https://wandb.ai/simranjeet97/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/simranjeet97/huggingface/runs/22owpe2g' target=\"_blank\">https://wandb.ai/simranjeet97/huggingface/runs/22owpe2g</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 00:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=30, training_loss=3.4812047322591146, metrics={'train_runtime': 62.4381, 'train_samples_per_second': 3.556, 'train_steps_per_second': 0.48, 'total_flos': 14501707776000.0, 'train_loss': 3.4812047322591146, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the model\nmodel.save_pretrained(\"./fine-tuned_model_gpt2_wiki\")\n\n# Save the tokenizer\ntokenizer.save_pretrained(\"./fine-tuned_model_gpt2_wiki_token\")","metadata":{"execution":{"iopub.status.busy":"2024-03-08T11:02:01.612466Z","iopub.execute_input":"2024-03-08T11:02:01.612850Z","iopub.status.idle":"2024-03-08T11:02:02.660276Z","shell.execute_reply.started":"2024-03-08T11:02:01.612822Z","shell.execute_reply":"2024-03-08T11:02:02.659236Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"('./fine-tuned_model_gpt2_wiki_token/tokenizer_config.json',\n './fine-tuned_model_gpt2_wiki_token/special_tokens_map.json',\n './fine-tuned_model_gpt2_wiki_token/vocab.json',\n './fine-tuned_model_gpt2_wiki_token/merges.txt',\n './fine-tuned_model_gpt2_wiki_token/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model\nfine_tuned_model = GPT2LMHeadModel.from_pretrained(\"./fine-tuned_model_gpt2_wiki\")\nfine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(\"./fine-tuned_model_gpt2_wiki_token\")\n\n# Text generation example\ngenerator = pipeline('text-generation', model=fine_tuned_model, tokenizer=fine_tuned_tokenizer)\ngenerated_text = generator(\"What is Python?\", max_length=100, num_return_sequences=1)[0]['generated_text']\n\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T11:02:29.479724Z","iopub.execute_input":"2024-03-08T11:02:29.480594Z","iopub.status.idle":"2024-03-08T11:02:34.937666Z","shell.execute_reply.started":"2024-03-08T11:02:29.480553Z","shell.execute_reply":"2024-03-08T11:02:34.936671Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"What is Python?\nPython allows a wide variety of applications to run in parallel, depending on the environment. Many of the most commonly used programming languages:\n\nPyartes\nJava (and many other languages)\nC++\nC# (as well as Java, Python and Perl itself)\nPython is a Java programming language, developed by the University of Chicago's Computer Science Center. It includes a Java runtime, and supports both standard and custom runtime systems.\nAn application programming language\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}