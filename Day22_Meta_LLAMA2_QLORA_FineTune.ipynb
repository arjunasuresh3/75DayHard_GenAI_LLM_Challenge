{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LLaMA 2 \nLlama 2 is a collection of second-generation open-source LLMs from Meta that comes with a commercial license. It is designed to handle a wide range of natural language processing tasks, with models ranging in scale from 7 billion to 70 billion parameters\n\n![LLAMA](https://images.datacamp.com/image/upload/v1697724450/Fine_Tune_L_La_MA_2_cc6aa0e4ad.png)","metadata":{}},{"cell_type":"markdown","source":"## Fine-tune the Llama 2 model with 7 billion parameters on a T4 GPU","metadata":{}},{"cell_type":"code","source":"# !pip install -U datasets trl accelerate peft bitsandbytes transformers trl huggingface_hub\n%pip install -U datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install accelerate peft bitsandbytes transformers trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging\n)\nfrom peft import LoraConfig, PeftModel\nfrom huggingface_hub import login\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:05:02.438998Z","iopub.execute_input":"2024-02-18T07:05:02.439307Z","iopub.status.idle":"2024-02-18T07:05:10.735349Z","shell.execute_reply.started":"2024-02-18T07:05:02.439280Z","shell.execute_reply":"2024-02-18T07:05:10.734336Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-18 07:05:07.450466: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-18 07:05:07.450519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-18 07:05:07.451996: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:05:18.812602Z","iopub.execute_input":"2024-02-18T07:05:18.813630Z","iopub.status.idle":"2024-02-18T07:05:20.145571Z","shell.execute_reply.started":"2024-02-18T07:05:18.813592Z","shell.execute_reply":"2024-02-18T07:05:20.144759Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Model from Hugging Face hub\nbase_model = \"NousResearch/Llama-2-7b-chat-hf\"\n\n# New instruction dataset\nguanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n\n# Fine-tuned model\nnew_model = \"llama-2-7b-chat-guanaco\"","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:05:22.715947Z","iopub.execute_input":"2024-02-18T07:05:22.716807Z","iopub.status.idle":"2024-02-18T07:05:22.721313Z","shell.execute_reply.started":"2024-02-18T07:05:22.716775Z","shell.execute_reply":"2024-02-18T07:05:22.720203Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(guanaco_dataset, split=\"train\") # MultiLingual Dataset\nprint(dataset['text'][1])","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:32:54.464223Z","iopub.execute_input":"2024-02-18T07:32:54.464962Z","iopub.status.idle":"2024-02-18T07:32:55.548120Z","shell.execute_reply.started":"2024-02-18T07:32:54.464924Z","shell.execute_reply":"2024-02-18T07:32:55.547090Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"<s>[INST] Самый великий человек из всех живших на планете? [/INST] Для начала нужно выбрать критерии величия человека. Обычно великим называют человека, который внес большой вклад в общество или сильно выделялся на фоне других в своем деле.\n\nНапример, Иосифа Бродского считают великим поэтом, а Иммануила Канта — великим философом. Александр Македонский, известный тем, что собрал в свои владения огромную империю (включавшую Македонию, Грецию, Персию, Египет), в историографии носит имя Александр Великий. Для христиан, скорее всего, самым великим человеком жившим на земле был Иисус Христос, так как он совершил множество благих деяний и совершил подвиг ради человечества. \n\nПри этом, когда мы выдвигаем одну личность на роль великого человека, сразу же находится множество людей, не согласных с этим. Того же Иосифа Бродского, хоть он и получил престижную Нобелевскую премию, некоторые люди считают графоманом и посредственным поэтом. \n\nВ целом, кого считать великим — это самостоятельный выбор каждого человека, который можно сделать только соотнося его со своими личными ценностями и представлениями о красивом, правильном, хорошем. </s>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 4-bit quantization configuration\n\n4-bit quantization via QLoRA allows efficient finetuning of huge LLM models on consumer hardware while retaining high performance. This dramatically improves accessibility and usability for real-world applications.\n\nQLoRA quantizes a pre-trained language model to 4 bits and freezes the parameters. A small number of trainable Low-Rank Adapter layers are then added to the model.\n\nDuring fine-tuning, gradients are backpropagated through the frozen 4-bit quantized model into only the Low-Rank Adapter layers. So, the entire pretrained model remains fixed at 4 bits while only the adapters are updated. Also, the 4-bit quantization does not hurt model performance.\n\nHere's a simplified example:\n\nOriginal Weight: 0.5487 (32-bit representation)\n\nQuantized Weight (4-bit): 0.5 (approximated for simplicity)\n\n![](https://images.datacamp.com/image/upload/v1697713094/image7_3e12912d0d.png)\n","metadata":{}},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True, # True to Make it QLORA and False for LORA\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=False,\n)\nquant_config","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:06:16.371322Z","iopub.execute_input":"2024-02-18T07:06:16.371747Z","iopub.status.idle":"2024-02-18T07:06:16.385083Z","shell.execute_reply.started":"2024-02-18T07:06:16.371714Z","shell.execute_reply":"2024-02-18T07:06:16.384002Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"BitsAndBytesConfig {\n  \"bnb_4bit_compute_dtype\": \"float16\",\n  \"bnb_4bit_quant_type\": \"nf4\",\n  \"bnb_4bit_use_double_quant\": false,\n  \"llm_int8_enable_fp32_cpu_offload\": false,\n  \"llm_int8_has_fp16_weight\": false,\n  \"llm_int8_skip_modules\": null,\n  \"llm_int8_threshold\": 6.0,\n  \"load_in_4bit\": true,\n  \"load_in_8bit\": false,\n  \"quant_method\": \"bitsandbytes\"\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"### Load the LLaMA 2 Model","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=quant_config,\n    device_map={\"\": 0}\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:06:37.513502Z","iopub.execute_input":"2024-02-18T07:06:37.513903Z","iopub.status.idle":"2024-02-18T07:07:55.004967Z","shell.execute_reply.started":"2024-02-18T07:06:37.513860Z","shell.execute_reply":"2024-02-18T07:07:55.003720Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea68b84dad54a16bb063337e19378fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"866051c1e4c542bcb7af9fbb66884be7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7d080383cf547a792f011fbc3271f30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e366de7b079414d95f6ad3fb3c79485"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cb37828e950471da7ea26eb6ede7648"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fde12f885d1e4866b70e53965f8aad60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4140ee19b414750a1dcbab3a89eecfe"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Loading the Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:07:55.007171Z","iopub.execute_input":"2024-02-18T07:07:55.008029Z","iopub.status.idle":"2024-02-18T07:07:56.046948Z","shell.execute_reply.started":"2024-02-18T07:07:55.007979Z","shell.execute_reply":"2024-02-18T07:07:56.046025Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1104e22b154424a981a94028b2f441e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d103e959b2bc4d6fadd18f1dd3657fb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e28fdceda7334a609c86c405f2c1d9a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa41d2d7d7b4a6eb8854a51e1ecc598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e95e3d2c2d742bea818077b59672abf"}},"metadata":{}}]},{"cell_type":"markdown","source":"### PEFT Parameters\n\nParameter-Efficient Fine-Tuning (PEFT) works by only updating a small subset of the model's parameters, making it much more efficient. ","metadata":{}},{"cell_type":"code","source":"peft_params = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:07:56.048140Z","iopub.execute_input":"2024-02-18T07:07:56.048444Z","iopub.status.idle":"2024-02-18T07:07:59.981486Z","shell.execute_reply.started":"2024-02-18T07:07:56.048417Z","shell.execute_reply":"2024-02-18T07:07:59.980186Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Training parameters","metadata":{}},{"cell_type":"code","source":"training_params = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,  # Start with 1 epoch and increase gradually if memory allows\n    per_device_train_batch_size=2,  # Begin with smallest batch size, increase in increments of 1\n    gradient_accumulation_steps=8,  # Aggressively accumulate gradients to compensate for low batch size\n    optim=\"adamw_torch\",  # Efficient optimizer for LLMs\n    save_steps=1000,  # Adjust saving frequency based on training duration\n    logging_steps=1000,  # Adjust logging frequency based on your preference\n    learning_rate=5e-6,  # Start with very low learning rate to mitigate instability\n    weight_decay=0.01,  # Regularization to prevent overfitting\n    fp16=True,  # Enable mixed precision for memory savings\n    bf16=False,  # T4 doesn't support bfloat16\n    max_grad_norm=0.5,  # Adjust gradient norm as needed\n    max_steps=-1,  # Train for all epochs by default\n    warmup_ratio=0.1,  # Adjust warmup ratio based on learning rate and dataset size\n    group_by_length=True,  # Improve efficiency for long sequences\n    lr_scheduler_type=\"constant\",  # Use warmup followed by constant learning rate\n    report_to=\"tensorboard\",  # Track training progress with TensorBoard\n    # Additional memory-specific optimizations:\n    #max_train_steps=1000,  # Set a maximum number of training steps to limit total memory usage\n    #sharded_ddp=True,  # Enable DistributedDataParallel sharding if multiple GPUs are available\n    gradient_checkpointing=True,  # Recompute intermediate activations for memory savings\n    fp16_full_eval=True,  # Use mixed precision during evaluation as well\n    dataloader_pin_memory=False,  # Disable data pinning to avoid potential memory overhead\n    local_rank=-1,  # Disable automatic distributed training (if only 1 GPU)\n    #skip_memory_check=True,  # Temporarily skip memory checks, but monitor closely\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:07:59.984686Z","iopub.execute_input":"2024-02-18T07:07:59.985197Z","iopub.status.idle":"2024-02-18T07:08:00.044887Z","shell.execute_reply.started":"2024-02-18T07:07:59.985151Z","shell.execute_reply":"2024-02-18T07:08:00.043661Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Model fine-tuning\n\nSupervised fine-tuning (SFT) is a key step in reinforcement learning from human feedback (RLHF). The TRL library from HuggingFace provides an easy-to-use API to create SFT models and train them on your dataset with just a few lines of code. It comes with tools to train language models using reinforcement learning, starting with supervised fine-tuning, then reward modeling, and finally proximal policy optimization (PPO).\n\nWe will provide SFT Trainer the model, dataset, Lora configuration, tokenizer, and training parameters.","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_params,\n    dataset_text_field=\"text\",\n    max_seq_length=None,\n    tokenizer=tokenizer,\n    args=training_params,\n    packing=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:08:00.048416Z","iopub.execute_input":"2024-02-18T07:08:00.048772Z","iopub.status.idle":"2024-02-18T07:08:01.651359Z","shell.execute_reply.started":"2024-02-18T07:08:00.048742Z","shell.execute_reply":"2024-02-18T07:08:01.650385Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14b34c9a49544478881c4234737e92a7"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:39:31.693401Z","iopub.execute_input":"2024-02-18T07:39:31.694297Z","iopub.status.idle":"2024-02-18T09:18:47.672785Z","shell.execute_reply.started":"2024-02-18T07:39:31.694260Z","shell.execute_reply":"2024-02-18T09:18:47.671761Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [31/31 1:33:05, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=31, training_loss=2.116789787046371, metrics={'train_runtime': 5954.8733, 'train_samples_per_second': 0.168, 'train_steps_per_second': 0.005, 'total_flos': 2.117766449351885e+16, 'train_loss': 2.116789787046371, 'epoch': 0.99})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.tokenizer.save_pretrained(new_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorboard import notebook\nlog_dir = \"results/runs\"\nnotebook.start(\"--logdir {} --port 4000\".format(log_dir))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\n\nprompt = \"Who is Leonardo Da Vinci?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:08:01.652513Z","iopub.execute_input":"2024-02-18T07:08:01.652827Z","iopub.status.idle":"2024-02-18T07:08:18.252559Z","shell.execute_reply.started":"2024-02-18T07:08:01.652799Z","shell.execute_reply":"2024-02-18T07:08:18.251529Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] Who is Leonardo Da Vinci? [/INST]  Leonardo da Vinci (1452-1519) was a true Renaissance man, a polymath who excelled in various fields, including art, science, engineering, mathematics, and anatomy. everybody knows him as the most famous artist of the Italian Renaissance, but he was also a prolific inventor, engineer, and scientist. Here are some key facts about Leonardo da Vinci:\n\n1. Early Life: Leonardo was born in Vinci, Italy, on April 15, 1452. His father, Messer Piero Fruosini, was a notary, and his mother, Caterina Buti, was a peasant.\n2. Artistic Career: Leonardo began his artistic career as a young man in Florence, where he was apprenticed to the artist Andrea del Ver\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"What is Data Science Career?\"\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-02-18T09:26:45.750415Z","iopub.execute_input":"2024-02-18T09:26:45.750866Z","iopub.status.idle":"2024-02-18T09:29:26.021335Z","shell.execute_reply.started":"2024-02-18T09:26:45.750831Z","shell.execute_reply":"2024-02-18T09:29:26.020227Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] What is Data Science Career? [/INST]  Data science is a field that combines mathematical and computational techniques to extract insights and knowledge from data. everybody in today's world is surrounded by data, from social media platforms to wearable devices, and data science professionals are in high demand to analyze and make sense of this data.\n\nA data science career typically involves working with large datasets to identify patterns, trends, and relationships, and using this information to inform business decisions or solve complex problems. Data scientists use a variety of tools and techniques, including machine learning algorithms, statistical modeling, and data visualization, to uncover insights and create value from data.\n\nSome common roles within the field of data science include:\n\n1. Data Scientist: A data scientist is responsible for collecting, analyzing, and interpreting large datasets to extract insights and make recommendations. They may work in a variety\n","output_type":"stream"}]},{"cell_type":"code","source":"!huggingface-cli login\n\nmodel.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
