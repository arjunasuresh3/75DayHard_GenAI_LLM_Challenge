{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you want to Learn more and need 1:1 Mentorship in Data Science, AI, Machine Learning and Generative AI.\n\nBook a call with Now : https://topmate.io/simranjeet97/743066**","metadata":{}},{"cell_type":"markdown","source":"# Quantization levels:\n\n1. During training the parameters learnt are in 32-bit floating point precision however for storage and deployment they are quantized to 16-bit floats (FP16) each. This leads to a minimal accuracy loss with considerable reduction in storage requirement.\n\n2. Quantizing the 32 bit float parameters to 8-bit integers (INT8). It reduces\nmemory requirement significantly but also causes noticeable degradation in model performance.\n\n3. A more extreme form of quantization is NF4. In this, 32-bit floats are scaled down to 4-bit integers or specialized data types like NormalFloat (NF4).\n\n# When to apply quantization?\n\n- **Post -Training Quantization (PTQ)**: Quantization can be applied after the LLM has been pre-trained. The pre-training parameters which are 32-bit floats are quantized to a lower precision.\n\n- **Quantization-Aware Training (QAT)**: Quantization can be applied during pre-training of the LLM. After defining the architecture of your model, you specify for which layers of the model the weights and activations will be quantized and to what target precision levels. Then during the training process, quantization aware training injects operations like scaling, clipping, rounding of parameter values. This leads to quantization errors in model calculations. This quantization error is added to the loss function and the optimizer now tries to update model weights to minimize the new loss. i.e., the modelâ€™s optimizer tries to adjust the weights not only to minimize the original loss from the data but also to minimize this additional quantization error. This helps the model become more robust to the eventual quantization after training. Finally, during fine-tuning, actual quantization (reducing the precision) is applied and the model is trained for some epochs to adapt to the quantized representation.\n\n# Most used Quantization Techniques for LLMs:\n1. LORA\n2. QLORA\n3. General Pre-Trained Transformer Quantization (GPTQ)\n4. Adaptive Weight Quantization\n\nBut, there is new Quantization Technique avaiable now.","metadata":{}},{"cell_type":"markdown","source":"Research Paper for 1.5Bit LLM - https://arxiv.org/pdf/2402.17764.pdf","metadata":{}},{"cell_type":"markdown","source":"# 1-Bit LLM and the 1.58 Bit LLM\n\n- 1-bit Transformer architecture for LLMs contains the Linear layer of the original transformer architecture with a new BiTLinear layer which enables the revised architecture to train 1-bit weights. \n\n- Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. \n\n- Just like the original Transformer architecture, the BitNet uses blocks of self-attention, FFNN. The matrix multiplication operation in BiTNet uses binarized model weights unlike conventional matrix multiplications.\n\n- First, the weights are centralized to zero-mean then they are binarized to +1 or -1 with the signum function. i.e., a positive weight maps to +1 and a negative weight to -1. \n\n- A scaling factor is used to reduce l2 error between the real valued and binary weights. Activations are quantized to b-bit precision. This architecture uses model parallelism i.e. the matrix multiplication operations are distributed across multiple machines.\n\n- The model was trained using straight-through estimator (STE) to compute the gradient during back propagation to bypass the non-differentiable functios like sign().\n\n![](https://miro.medium.com/v2/resize:fit:1266/format:webp/1*-Wtae6KYMuZgFmxAIUoQSA.png)","metadata":{}},{"cell_type":"markdown","source":"# 1.5Bit LLM\n- 1.5 Bit LLM introduced a variant of above 1-Bit architecture that stores every parameter of the LLM in a ternary representation [-1, 0, 1] i.e., a weight can be +1, 0 or -1. \n\n- The weight matrix being only -1, 0 and +1 means that the matrix multiplication operations that normal transformer models do, are replaced by simple addition (and subtraction) operations. \n\n- This gives the new architecture strong memory and computational cost savings as compared to the conventional transformer based LLMs.","metadata":{}},{"cell_type":"markdown","source":"## Now, Do you know Why 1Bit or 1.5Bit Quantization does not loss information as comapred to INT1 and INT4 Quantization?","metadata":{}},{"cell_type":"markdown","source":"**Binary quantization** represents the extreme case of quantization where numerical values are compressed into binary representations (-1s and 1s). In this scenario, information loss is minimized because each value is either fully represented (as -1 or 1) or not represented at all(0s) by capturing the direction of the value not the magnitude of the value. \n\nHowever, in more **granular quantization schemes like INT4 and INT1**, where values are mapped to a small set of discrete integer values, information loss can occur because the representation cannot capture the full range of original values.\n\n![](https://pbs.twimg.com/media/GJnC7IOXEAAH1MA.jpg)","metadata":{}},{"cell_type":"markdown","source":"- BitNet b1.58 matches 16-bit Floating point LLM baselines in perplexity and end-task performance.\n\n- It provides faster processing speeds and uses less GPU memory compared to traditional models.\n\n- The model minimizes multiplication operations for matrix multiplication, enhancing optimization and efficiency.\n\n- Includes a quantization function for system-level optimization and integrates components like RMSNorm and SwiGLU similar to LLaMA.","metadata":{}},{"cell_type":"markdown","source":"# Watch Practical Implementation of Quantization Techniques and Fine Tuning of LLM\n\n- LORA and QLORA Fine Tuning Explained - https://youtu.be/5MeqxrpOc0s\n- Databricks LLM Fine Tuning LORA - https://youtu.be/SX4UAz7AYic\n- Google Gemma LLM Fine Tuning LORA - https://youtu.be/9pJ85vLVRk4\n- META LLAMA 7B Fine Tuning QLORA - https://youtu.be/gt6q5O44c7E\n- Microsoft Phi-2 Fine Tuning - QLORA - https://youtu.be/lDSxeUBo-6E\n\n# If you want to Learn more and need 1:1 Mentorship in Data Science, AI, Machine Learning and Generative AI.\n\n## Book a call with Now : https://topmate.io/simranjeet97/743066**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}