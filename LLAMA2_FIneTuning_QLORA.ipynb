{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LLaMA 2 \nLlama 2 is a collection of second-generation open-source LLMs from Meta that comes with a commercial license. It is designed to handle a wide range of natural language processing tasks, with models ranging in scale from 7 billion to 70 billion parameters\n\n![LLAMA](https://images.datacamp.com/image/upload/v1697724450/Fine_Tune_L_La_MA_2_cc6aa0e4ad.png)","metadata":{}},{"cell_type":"markdown","source":"## Fine-tune the Llama 2 model with 7 billion parameters on a T4 GPU","metadata":{}},{"cell_type":"code","source":"# !pip install -U datasets trl accelerate peft bitsandbytes transformers trl huggingface_hub\n%pip install -U datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install accelerate peft bitsandbytes transformers trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging\n)\nfrom peft import LoraConfig, PeftModel\nfrom huggingface_hub import login\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:39:06.967417Z","iopub.execute_input":"2024-02-14T16:39:06.967742Z","iopub.status.idle":"2024-02-14T16:39:14.730068Z","shell.execute_reply.started":"2024-02-14T16:39:06.967713Z","shell.execute_reply":"2024-02-14T16:39:14.729167Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-14 16:39:11.646660: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-14 16:39:11.646712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-14 16:39:11.648236: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:39:14.731827Z","iopub.execute_input":"2024-02-14T16:39:14.732578Z","iopub.status.idle":"2024-02-14T16:39:15.673386Z","shell.execute_reply.started":"2024-02-14T16:39:14.732544Z","shell.execute_reply":"2024-02-14T16:39:15.672592Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Model from Hugging Face hub\nbase_model = \"NousResearch/Llama-2-7b-chat-hf\"\n\n# New instruction dataset\nguanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n\n# Fine-tuned model\nnew_model = \"llama-2-7b-chat-guanaco\"","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:39:21.240806Z","iopub.execute_input":"2024-02-14T16:39:21.241208Z","iopub.status.idle":"2024-02-14T16:39:21.246163Z","shell.execute_reply.started":"2024-02-14T16:39:21.241179Z","shell.execute_reply":"2024-02-14T16:39:21.245039Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(guanaco_dataset, split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:40:03.327351Z","iopub.execute_input":"2024-02-14T16:40:03.327744Z","iopub.status.idle":"2024-02-14T16:40:05.945885Z","shell.execute_reply.started":"2024-02-14T16:40:03.327714Z","shell.execute_reply":"2024-02-14T16:40:05.945007Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5ec57b98236401c8c557a2a6282205d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/967k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03840546172e4f3e81bb7ddff272f6c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c336fb2fc64509928a625fd97470b3"}},"metadata":{}}]},{"cell_type":"markdown","source":"## 4-bit quantization configuration\n\n4-bit quantization via QLoRA allows efficient finetuning of huge LLM models on consumer hardware while retaining high performance. This dramatically improves accessibility and usability for real-world applications.\n\nQLoRA quantizes a pre-trained language model to 4 bits and freezes the parameters. A small number of trainable Low-Rank Adapter layers are then added to the model.\n\nDuring fine-tuning, gradients are backpropagated through the frozen 4-bit quantized model into only the Low-Rank Adapter layers. So, the entire pretrained model remains fixed at 4 bits while only the adapters are updated. Also, the 4-bit quantization does not hurt model performance.\n\n![](https://images.datacamp.com/image/upload/v1697713094/image7_3e12912d0d.png)\n","metadata":{}},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:40:10.443983Z","iopub.execute_input":"2024-02-14T16:40:10.444832Z","iopub.status.idle":"2024-02-14T16:40:10.451930Z","shell.execute_reply.started":"2024-02-14T16:40:10.444795Z","shell.execute_reply":"2024-02-14T16:40:10.450748Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Load the LLaMA 2 Model","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=quant_config,\n    device_map={\"\": 0}\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:40:12.131638Z","iopub.execute_input":"2024-02-14T16:40:12.132016Z","iopub.status.idle":"2024-02-14T16:41:41.504431Z","shell.execute_reply.started":"2024-02-14T16:40:12.131986Z","shell.execute_reply":"2024-02-14T16:41:41.503410Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf2e6443c974c8eb9261fed0eaccf2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffa0e5591503457dae56e257a8508ad9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e193ffa19b742139313536c9040af8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50a198795e1e47fdb712bb1f86696e9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"894b0beb6f5d422e88145ffa56472059"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db5ac07dde9441c4bffa69fefd88b301"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7537b1ccf544972801a4115ff916695"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Loading the Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:41:53.206439Z","iopub.execute_input":"2024-02-14T16:41:53.207354Z","iopub.status.idle":"2024-02-14T16:41:54.257768Z","shell.execute_reply.started":"2024-02-14T16:41:53.207316Z","shell.execute_reply":"2024-02-14T16:41:54.256309Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4274b9ed64a40de9ac55f4998160b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02390850039d41c1b053a5f24c850d21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f1f268619743b69e4d8cefa8fdc106"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c5e22724d144248abd7fcb1c25366c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ce49a86189f426698d67b6f22ff9859"}},"metadata":{}}]},{"cell_type":"markdown","source":"### PEFT Parameters\n\nParameter-Efficient Fine-Tuning (PEFT) works by only updating a small subset of the model's parameters, making it much more efficient. ","metadata":{}},{"cell_type":"code","source":"peft_params = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:41:56.769828Z","iopub.execute_input":"2024-02-14T16:41:56.770588Z","iopub.status.idle":"2024-02-14T16:41:56.775189Z","shell.execute_reply.started":"2024-02-14T16:41:56.770552Z","shell.execute_reply":"2024-02-14T16:41:56.774097Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Training parameters","metadata":{}},{"cell_type":"code","source":"training_params = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,  # Start with 1 epoch and increase gradually if memory allows\n    per_device_train_batch_size=2,  # Begin with smallest batch size, increase in increments of 1\n    gradient_accumulation_steps=8,  # Aggressively accumulate gradients to compensate for low batch size\n    optim=\"adamw_torch\",  # Efficient optimizer for LLMs\n    save_steps=1000,  # Adjust saving frequency based on training duration\n    logging_steps=1000,  # Adjust logging frequency based on your preference\n    learning_rate=5e-6,  # Start with very low learning rate to mitigate instability\n    weight_decay=0.01,  # Regularization to prevent overfitting\n    fp16=True,  # Enable mixed precision for memory savings\n    bf16=False,  # T4 doesn't support bfloat16\n    max_grad_norm=0.5,  # Adjust gradient norm as needed\n    max_steps=-1,  # Train for all epochs by default\n    warmup_ratio=0.1,  # Adjust warmup ratio based on learning rate and dataset size\n    group_by_length=True,  # Improve efficiency for long sequences\n    lr_scheduler_type=\"constant\",  # Use warmup followed by constant learning rate\n    report_to=\"tensorboard\",  # Track training progress with TensorBoard\n    # Additional memory-specific optimizations:\n    #max_train_steps=1000,  # Set a maximum number of training steps to limit total memory usage\n    #sharded_ddp=True,  # Enable DistributedDataParallel sharding if multiple GPUs are available\n    gradient_checkpointing=True,  # Recompute intermediate activations for memory savings\n    fp16_full_eval=True,  # Use mixed precision during evaluation as well\n    dataloader_pin_memory=False,  # Disable data pinning to avoid potential memory overhead\n    local_rank=-1,  # Disable automatic distributed training (if only 1 GPU)\n    #skip_memory_check=True,  # Temporarily skip memory checks, but monitor closely\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:42:02.889338Z","iopub.execute_input":"2024-02-14T16:42:02.890103Z","iopub.status.idle":"2024-02-14T16:42:02.899129Z","shell.execute_reply.started":"2024-02-14T16:42:02.890068Z","shell.execute_reply":"2024-02-14T16:42:02.898261Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Model fine-tuning\n\nSupervised fine-tuning (SFT) is a key step in reinforcement learning from human feedback (RLHF). The TRL library from HuggingFace provides an easy-to-use API to create SFT models and train them on your dataset with just a few lines of code. It comes with tools to train language models using reinforcement learning, starting with supervised fine-tuning, then reward modeling, and finally proximal policy optimization (PPO).\n\nWe will provide SFT Trainer the model, dataset, Lora configuration, tokenizer, and training parameters.","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_params,\n    dataset_text_field=\"text\",\n    max_seq_length=None,\n    tokenizer=tokenizer,\n    args=training_params,\n    packing=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:42:06.227413Z","iopub.execute_input":"2024-02-14T16:42:06.227773Z","iopub.status.idle":"2024-02-14T16:42:07.539745Z","shell.execute_reply.started":"2024-02-14T16:42:06.227747Z","shell.execute_reply":"2024-02-14T16:42:07.538927Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c25fe05ddaa04998b6955a345506cecf"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.tokenizer.save_pretrained(new_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorboard import notebook\nlog_dir = \"results/runs\"\nnotebook.start(\"--logdir {} --port 4000\".format(log_dir))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\n\nprompt = \"Who is Leonardo Da Vinci?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:44:13.972766Z","iopub.execute_input":"2024-02-14T16:44:13.973171Z","iopub.status.idle":"2024-02-14T16:44:29.474841Z","shell.execute_reply.started":"2024-02-14T16:44:13.973139Z","shell.execute_reply":"2024-02-14T16:44:29.473775Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"<s>[INST] Who is Leonardo Da Vinci? [/INST]  Leonardo da Vinci (1452-1519) was a true Renaissance man, a polymath who excelled in various fields, including art, science, engineering, mathematics, and anatomy. everybody knows him as the most famous artist of the Italian Renaissance, but he was also a prolific inventor, engineer, and scientist. Here are some key facts about Leonardo da Vinci:\n\n1. Early Life: Leonardo was born in Vinci, Italy, on April 15, 1452. His father, Messer Piero Fruosini, was a notary, and his mother, Caterina Buti, was a peasant.\n2. Artistic Career: Leonardo began his artistic career as a young man in Florence, where he was apprenticed to the artist Andrea del Ver\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"What is Datacamp Career track?\"\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:43:30.948272Z","iopub.execute_input":"2024-02-14T16:43:30.948647Z","iopub.status.idle":"2024-02-14T16:43:46.232590Z","shell.execute_reply.started":"2024-02-14T16:43:30.948616Z","shell.execute_reply":"2024-02-14T16:43:46.231585Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"<s>[INST] What is Datacamp Career track? [/INST]  DataCamp is an online learning platform that offers a range of data science and machine learning courses and tracks. everybody can learn from DataCamp's courses, but the platform also offers a Career Track program, which is designed for individuals who want to take their data science skills to the next level and prepare for a career in the field.\n\nThe DataCamp Career Track is a comprehensive program that provides learners with the skills and knowledge they need to succeed in a data science career. The program includes a range of courses and projects that cover the key concepts and skills required in the field, including:\n\n1. Data Wrangling and Visualization: Learn how to work with different data types, clean and transform data, and create effective visualizations.\n2. Machine Learning Fundamentals: Understand the basics of machine learning, including supervised and unsupervised learning\n","output_type":"stream"}]},{"cell_type":"code","source":"!huggingface-cli login\n\nmodel.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T16:42:32.949886Z","iopub.status.idle":"2024-02-14T16:42:32.950282Z","shell.execute_reply.started":"2024-02-14T16:42:32.950095Z","shell.execute_reply":"2024-02-14T16:42:32.950111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}