{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Day - 4**\n",
        "## **Optimization Techniques**\n",
        "\n",
        "## Working Code of **Stochastic Gradient Descent (SGD)** using Python and Tensorflow\n",
        "\n",
        "**Video Link -** https://youtu.be/6wAKUqzgOms"
      ],
      "metadata": {
        "id": "SYJdjQGf817i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjJoTqfR8nij",
        "outputId": "85e6c5b2-a07e-461f-d70d-08cc889a2eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ------------- Data Creation Done ----------------- \n",
            " -------------Epoches Model Training----------------- \n",
            "Epoch 0, Loss: 15.18087387084961\n",
            "Epoch 10, Loss: 9.341418266296387\n",
            "Epoch 20, Loss: 5.800542831420898\n",
            "Epoch 30, Loss: 3.6519951820373535\n",
            "Epoch 40, Loss: 2.346867322921753\n",
            "Epoch 50, Loss: 1.5526927709579468\n",
            "Epoch 60, Loss: 1.06809663772583\n",
            "Epoch 70, Loss: 0.771107017993927\n",
            "Epoch 80, Loss: 0.5878450870513916\n",
            "Epoch 90, Loss: 0.4735606908798218\n",
            " ---------------------------------------- \n",
            "Mean Absolute Error (MAE): 0.5261561274528503\n",
            "R-squared (R2) Score: 0.48079368280965806\n",
            " ---------------------------------------- \n",
            "Learned parameters:\n",
            "[array([[1.0056099]], dtype=float32), array([2.6917145], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Function to generate dummy data\n",
        "def generate_dummy_data():\n",
        "    np.random.seed(42)\n",
        "    X_train = np.random.rand(100, 1)  # 100 samples, 1 feature\n",
        "    y_train = 3 * X_train + 2 + 0.1 * np.random.randn(100, 1)  # Linear relationship with some noise\n",
        "    return X_train.astype(np.float32), y_train.astype(np.float32)\n",
        "\n",
        "# Function to create a simple linear regression model\n",
        "def create_neural_network():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(1,)),\n",
        "        tf.keras.layers.Dense(units=1)  # Linear layer with one unit for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Loss function for regression (Mean Squared Error)\n",
        "def loss_function(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "# Dummy data\n",
        "print(\" ------------- Data Creation Done ----------------- \")\n",
        "X_train, y_train = generate_dummy_data()\n",
        "\n",
        "# Model definition\n",
        "model = create_neural_network()\n",
        "\n",
        "# Optimizer setup\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "print(\" -------------Epoches Model Training----------------- \")\n",
        "for epoch in range(num_epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(X_train)\n",
        "        loss = loss_function(y_train, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.numpy()}')\n",
        "\n",
        "\n",
        "# Evaluate model performance\n",
        "predictions = model(X_train)\n",
        "\n",
        "# Convert predictions and true values to numpy arrays\n",
        "y_pred_np = predictions.numpy().flatten()\n",
        "y_true_np = y_train.flatten()\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE) and R-squared (R2) score\n",
        "mae = mean_absolute_error(y_true_np, y_pred_np)\n",
        "r2 = r2_score(y_true_np, y_pred_np)\n",
        "\n",
        "print(\" ---------------------------------------- \")\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'R-squared (R2) Score: {r2}')\n",
        "print(\" ---------------------------------------- \")\n",
        "\n",
        "# Check the learned parameters\n",
        "print(\"Learned parameters:\")\n",
        "for layer in model.layers:\n",
        "    print(layer.get_weights())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ADAM Optimizer**"
      ],
      "metadata": {
        "id": "vmhJNx0nD0Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Function to generate dummy data\n",
        "def generate_dummy_data():\n",
        "    np.random.seed(42)\n",
        "    X_train = np.random.rand(100, 1)  # 100 samples, 1 feature\n",
        "    y_train = 3 * X_train + 2 + 0.1 * np.random.randn(100, 1)  # Linear relationship with some noise\n",
        "    return X_train.astype(np.float32), y_train.astype(np.float32)\n",
        "\n",
        "# Function to create a simple linear regression model\n",
        "def create_neural_network():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(1,)),\n",
        "        tf.keras.layers.Dense(units=1)  # Linear layer with one unit for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Loss function for regression (Mean Squared Error)\n",
        "def loss_function(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "# Dummy data\n",
        "X_train, y_train = generate_dummy_data()\n",
        "\n",
        "# Model definition\n",
        "model_adam = create_neural_network()\n",
        "\n",
        "# Optimizer setup\n",
        "learning_rate_adam = 0.01\n",
        "\n",
        "optimizer_adam = tf.keras.optimizers.Adam(learning_rate=learning_rate_adam, beta_1=0.01, beta_2=0.1, epsilon=1e-05)\n",
        "\n",
        "# Training loop for Adam\n",
        "for epoch in range(num_epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model_adam(X_train)\n",
        "        loss = loss_function(y_train, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model_adam.trainable_variables)\n",
        "    optimizer_adam.apply_gradients(zip(gradients, model_adam.trainable_variables))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Adam - Epoch {epoch}, Loss: {loss.numpy()}')\n",
        "\n",
        "# Evaluate model performance for Adam\n",
        "predictions_adam = model_adam(X_train)\n",
        "y_pred_adam = predictions_adam.numpy().flatten()\n",
        "mae_adam = mean_absolute_error(y_train.flatten(), y_pred_adam)\n",
        "r2_adam = r2_score(y_train.flatten(), y_pred_adam)\n",
        "print('\\nAdam Results:')\n",
        "print(f'Mean Absolute Error (MAE): {mae_adam}')\n",
        "print(f'R-squared (R2) Score: {r2_adam}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxMjZjkN9Hd-",
        "outputId": "bfead001-573c-4d3e-dddd-d34ba5504294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adam - Epoch 0, Loss: 7.439562797546387\n",
            "Adam - Epoch 10, Loss: 6.645547389984131\n",
            "Adam - Epoch 20, Loss: 5.896543502807617\n",
            "Adam - Epoch 30, Loss: 5.1924920082092285\n",
            "Adam - Epoch 40, Loss: 4.533390998840332\n",
            "Adam - Epoch 50, Loss: 3.9192392826080322\n",
            "Adam - Epoch 60, Loss: 3.3500332832336426\n",
            "Adam - Epoch 70, Loss: 2.825770854949951\n",
            "Adam - Epoch 80, Loss: 2.3464479446411133\n",
            "Adam - Epoch 90, Loss: 1.9120614528656006\n",
            "\n",
            "Adam Results:\n",
            "Mean Absolute Error (MAE): 1.2240923643112183\n",
            "R-squared (R2) Score: 0.9707154198241725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DAY-5**\n",
        "## **NLP Stage-1**\n",
        "- What is NLP?\n",
        "- Significance of NLP?\n",
        "- Why NLP in Generative AI?\n",
        "- Types of NLP Algorithms\n",
        "- Latest Advancements in NLP.\n",
        "\n",
        "**Video Link** - https://youtu.be/CBtHgX-DHwk"
      ],
      "metadata": {
        "id": "vpoelzFWvs3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Latest Advancements in Natural Language Processing until Jan 2024**\n",
        "\n",
        "The world of NLP and generative AI is constantly evolving, pushing the boundaries of what these technologies can achieve. Here are some of the latest advancements, with a focus on generative AI:\n",
        "\n",
        "### **Model advancements:**\n",
        "\n",
        "- **PaLM 540B**: Google's behemoth model boasts impressive performance across reasoning tasks and benchmarks.\n",
        "\n",
        "- **Megatron-Turing NLG**: Microsoft's 530B parameter model specializes in creative text formats like poetry, code, and scripts.\n",
        "\n",
        "- **Jurassic-1 Jumbo**: This 178B parameter LLM excels at factual language tasks like question answering and summarization.\n",
        "\n",
        "- **BARD**: Google AI's new model with a focus on factual language, code generation, and open-ended creative tasks.\n",
        "\n",
        "### **Technical breakthroughs:**\n",
        "\n",
        "- **Diffusion models**: These models, like Disco Diffusion v5, are revolutionizing image generation with greater control and artistic direction.\n",
        "\n",
        "- **Reinforcement learning (RL):** Emerging advancements in RL are improving dialogue generation and machine translation.\n",
        "\n",
        "- **Multimodal NLP**: Models are now integrating text with other modalities like images and audio, leading to richer and more context-aware results.\n",
        "\n",
        "### **Specific applications:**\n",
        "\n",
        "- **AI-powered music and art:** Tools like MuseNet and Jukebox are creating stunning and original compositions while pushing the boundaries of artistic expression.\n",
        "\n",
        "- **Generative AI in scientific discovery:**AI models are assisting with data analysis, hypothesis generation, and research acceleration.\n",
        "\n",
        "- **Enhanced chatbots and virtual assistants:**LLMs are making these tools more sophisticated and engaging, enabling natural and informative interactions.\n",
        "\n",
        "- **Personalized content creation:** Generative AI can tailor marketing copy, educational materials, and even product descriptions to specific audiences.\n",
        "\n",
        "### **Ethical considerations:**\n",
        "\n",
        "- **Deepfakes and misinformation**: Malicious use of generative AI to create fake content raises concerns about authenticity and trust.\n",
        "\n",
        "- **Bias and fairness:** Models can inherit biases from training data, necessitating careful data selection and mitigation strategies.\n",
        "\n",
        "- **Explainability and interpretability**: Understanding how models generate outputs is crucial for responsible development and use.\n",
        "\n",
        "### **Trends to watch:**\n",
        "\n",
        "- **Open-source models**: Initiatives like Bloom aim to democratize access and transparency in LLM development.\n",
        "\n",
        "- **Hybrid human-AI collaboration**: Combining human creativity with AI capabilities can lead to even more impressive outcomes.\n",
        "\n",
        "- **Focus on responsible and ethical AI:** As the field matures, ethical considerations and responsible development practices are becoming increasingly important."
      ],
      "metadata": {
        "id": "LKnvVtAtyzzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QWEaMnaCD4dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AkHOsgJgy5aj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}