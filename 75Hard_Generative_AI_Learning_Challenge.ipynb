{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Day - 4**\n",
        "## **Optimization Techniques**\n",
        "\n",
        "## Working Code of **Stochastic Gradient Descent (SGD)** using Python and Tensorflow\n",
        "\n",
        "**Video Link -** https://youtu.be/6wAKUqzgOms"
      ],
      "metadata": {
        "id": "SYJdjQGf817i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjJoTqfR8nij",
        "outputId": "85e6c5b2-a07e-461f-d70d-08cc889a2eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ------------- Data Creation Done ----------------- \n",
            " -------------Epoches Model Training----------------- \n",
            "Epoch 0, Loss: 15.18087387084961\n",
            "Epoch 10, Loss: 9.341418266296387\n",
            "Epoch 20, Loss: 5.800542831420898\n",
            "Epoch 30, Loss: 3.6519951820373535\n",
            "Epoch 40, Loss: 2.346867322921753\n",
            "Epoch 50, Loss: 1.5526927709579468\n",
            "Epoch 60, Loss: 1.06809663772583\n",
            "Epoch 70, Loss: 0.771107017993927\n",
            "Epoch 80, Loss: 0.5878450870513916\n",
            "Epoch 90, Loss: 0.4735606908798218\n",
            " ---------------------------------------- \n",
            "Mean Absolute Error (MAE): 0.5261561274528503\n",
            "R-squared (R2) Score: 0.48079368280965806\n",
            " ---------------------------------------- \n",
            "Learned parameters:\n",
            "[array([[1.0056099]], dtype=float32), array([2.6917145], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Function to generate dummy data\n",
        "def generate_dummy_data():\n",
        "    np.random.seed(42)\n",
        "    X_train = np.random.rand(100, 1)  # 100 samples, 1 feature\n",
        "    y_train = 3 * X_train + 2 + 0.1 * np.random.randn(100, 1)  # Linear relationship with some noise\n",
        "    return X_train.astype(np.float32), y_train.astype(np.float32)\n",
        "\n",
        "# Function to create a simple linear regression model\n",
        "def create_neural_network():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(1,)),\n",
        "        tf.keras.layers.Dense(units=1)  # Linear layer with one unit for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Loss function for regression (Mean Squared Error)\n",
        "def loss_function(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "# Dummy data\n",
        "print(\" ------------- Data Creation Done ----------------- \")\n",
        "X_train, y_train = generate_dummy_data()\n",
        "\n",
        "# Model definition\n",
        "model = create_neural_network()\n",
        "\n",
        "# Optimizer setup\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "print(\" -------------Epoches Model Training----------------- \")\n",
        "for epoch in range(num_epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(X_train)\n",
        "        loss = loss_function(y_train, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.numpy()}')\n",
        "\n",
        "\n",
        "# Evaluate model performance\n",
        "predictions = model(X_train)\n",
        "\n",
        "# Convert predictions and true values to numpy arrays\n",
        "y_pred_np = predictions.numpy().flatten()\n",
        "y_true_np = y_train.flatten()\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE) and R-squared (R2) score\n",
        "mae = mean_absolute_error(y_true_np, y_pred_np)\n",
        "r2 = r2_score(y_true_np, y_pred_np)\n",
        "\n",
        "print(\" ---------------------------------------- \")\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'R-squared (R2) Score: {r2}')\n",
        "print(\" ---------------------------------------- \")\n",
        "\n",
        "# Check the learned parameters\n",
        "print(\"Learned parameters:\")\n",
        "for layer in model.layers:\n",
        "    print(layer.get_weights())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ADAM Optimizer**"
      ],
      "metadata": {
        "id": "vmhJNx0nD0Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Function to generate dummy data\n",
        "def generate_dummy_data():\n",
        "    np.random.seed(42)\n",
        "    X_train = np.random.rand(100, 1)  # 100 samples, 1 feature\n",
        "    y_train = 3 * X_train + 2 + 0.1 * np.random.randn(100, 1)  # Linear relationship with some noise\n",
        "    return X_train.astype(np.float32), y_train.astype(np.float32)\n",
        "\n",
        "# Function to create a simple linear regression model\n",
        "def create_neural_network():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(1,)),\n",
        "        tf.keras.layers.Dense(units=1)  # Linear layer with one unit for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Loss function for regression (Mean Squared Error)\n",
        "def loss_function(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "# Dummy data\n",
        "X_train, y_train = generate_dummy_data()\n",
        "\n",
        "# Model definition\n",
        "model_adam = create_neural_network()\n",
        "\n",
        "# Optimizer setup\n",
        "learning_rate_adam = 0.01\n",
        "\n",
        "optimizer_adam = tf.keras.optimizers.Adam(learning_rate=learning_rate_adam, beta_1=0.01, beta_2=0.1, epsilon=1e-05)\n",
        "\n",
        "# Training loop for Adam\n",
        "for epoch in range(num_epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model_adam(X_train)\n",
        "        loss = loss_function(y_train, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model_adam.trainable_variables)\n",
        "    optimizer_adam.apply_gradients(zip(gradients, model_adam.trainable_variables))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Adam - Epoch {epoch}, Loss: {loss.numpy()}')\n",
        "\n",
        "# Evaluate model performance for Adam\n",
        "predictions_adam = model_adam(X_train)\n",
        "y_pred_adam = predictions_adam.numpy().flatten()\n",
        "mae_adam = mean_absolute_error(y_train.flatten(), y_pred_adam)\n",
        "r2_adam = r2_score(y_train.flatten(), y_pred_adam)\n",
        "print('\\nAdam Results:')\n",
        "print(f'Mean Absolute Error (MAE): {mae_adam}')\n",
        "print(f'R-squared (R2) Score: {r2_adam}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxMjZjkN9Hd-",
        "outputId": "bfead001-573c-4d3e-dddd-d34ba5504294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adam - Epoch 0, Loss: 7.439562797546387\n",
            "Adam - Epoch 10, Loss: 6.645547389984131\n",
            "Adam - Epoch 20, Loss: 5.896543502807617\n",
            "Adam - Epoch 30, Loss: 5.1924920082092285\n",
            "Adam - Epoch 40, Loss: 4.533390998840332\n",
            "Adam - Epoch 50, Loss: 3.9192392826080322\n",
            "Adam - Epoch 60, Loss: 3.3500332832336426\n",
            "Adam - Epoch 70, Loss: 2.825770854949951\n",
            "Adam - Epoch 80, Loss: 2.3464479446411133\n",
            "Adam - Epoch 90, Loss: 1.9120614528656006\n",
            "\n",
            "Adam Results:\n",
            "Mean Absolute Error (MAE): 1.2240923643112183\n",
            "R-squared (R2) Score: 0.9707154198241725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DAY-5**\n",
        "## **NLP Stage-1**\n",
        "- What is NLP?\n",
        "- Significance of NLP?\n",
        "- Why NLP in Generative AI?\n",
        "- Types of NLP Algorithms\n",
        "- Latest Advancements in NLP.\n",
        "\n",
        "**Video Link** - https://youtu.be/CBtHgX-DHwk"
      ],
      "metadata": {
        "id": "vpoelzFWvs3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Latest Advancements in Natural Language Processing until Jan 2024**\n",
        "\n",
        "The world of NLP and generative AI is constantly evolving, pushing the boundaries of what these technologies can achieve. Here are some of the latest advancements, with a focus on generative AI:\n",
        "\n",
        "### **Model advancements:**\n",
        "\n",
        "- **PaLM 540B**: Google's behemoth model boasts impressive performance across reasoning tasks and benchmarks.\n",
        "\n",
        "- **Megatron-Turing NLG**: Microsoft's 530B parameter model specializes in creative text formats like poetry, code, and scripts.\n",
        "\n",
        "- **Jurassic-1 Jumbo**: This 178B parameter LLM excels at factual language tasks like question answering and summarization.\n",
        "\n",
        "- **BARD**: Google AI's new model with a focus on factual language, code generation, and open-ended creative tasks.\n",
        "\n",
        "### **Technical breakthroughs:**\n",
        "\n",
        "- **Diffusion models**: These models, like Disco Diffusion v5, are revolutionizing image generation with greater control and artistic direction.\n",
        "\n",
        "- **Reinforcement learning (RL):** Emerging advancements in RL are improving dialogue generation and machine translation.\n",
        "\n",
        "- **Multimodal NLP**: Models are now integrating text with other modalities like images and audio, leading to richer and more context-aware results.\n",
        "\n",
        "### **Specific applications:**\n",
        "\n",
        "- **AI-powered music and art:** Tools like MuseNet and Jukebox are creating stunning and original compositions while pushing the boundaries of artistic expression.\n",
        "\n",
        "- **Generative AI in scientific discovery:**AI models are assisting with data analysis, hypothesis generation, and research acceleration.\n",
        "\n",
        "- **Enhanced chatbots and virtual assistants:**LLMs are making these tools more sophisticated and engaging, enabling natural and informative interactions.\n",
        "\n",
        "- **Personalized content creation:** Generative AI can tailor marketing copy, educational materials, and even product descriptions to specific audiences.\n",
        "\n",
        "### **Ethical considerations:**\n",
        "\n",
        "- **Deepfakes and misinformation**: Malicious use of generative AI to create fake content raises concerns about authenticity and trust.\n",
        "\n",
        "- **Bias and fairness:** Models can inherit biases from training data, necessitating careful data selection and mitigation strategies.\n",
        "\n",
        "- **Explainability and interpretability**: Understanding how models generate outputs is crucial for responsible development and use.\n",
        "\n",
        "### **Trends to watch:**\n",
        "\n",
        "- **Open-source models**: Initiatives like Bloom aim to democratize access and transparency in LLM development.\n",
        "\n",
        "- **Hybrid human-AI collaboration**: Combining human creativity with AI capabilities can lead to even more impressive outcomes.\n",
        "\n",
        "- **Focus on responsible and ethical AI:** As the field matures, ethical considerations and responsible development practices are becoming increasingly important."
      ],
      "metadata": {
        "id": "LKnvVtAtyzzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DAY-6**\n",
        "## **NLP Stage-2**\n",
        "- Data Pre-Processing Techniques Explained\n",
        "\n",
        "**Video Link** -\n",
        "\n",
        "Data Pre-Processing means changing the raw data into a clean data set. The dataset is preprocessed in order to check missing values, noisy data, and other inconsistencies before executing it to the algorithm.\n",
        "\n",
        "In NLP, Data Pre-Processing Techniques make the raw textual data into numerical form so that it is easy to understand by algorithms because algorithms understand numerical data only.\n",
        "So data needs to be transformed in such a way that it's meaning should keep intact and also it should be converted into numerical form.\n",
        "\n",
        "## **1.Tokeniztion**\n",
        "Tokenization in NLP involves breaking down a text into smaller units, which are typically words or subwords. The resulting units are called tokens. Tokenization is a crucial step in language processing because it transforms raw text into a format that is suitable for analysis and machine learning.\n",
        "\n",
        "![Tokenization](data:image/png;base64,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)\n",
        "\n",
        "### **Why Tokenization?**\n",
        "- Text Understanding\n",
        "- Feature Extraction\n",
        "- Text Analysis\n",
        "\n",
        "### **Types of Tokenization**\n",
        "- Word Tokenization - Breaks text into words based on spaces and punctuation. Example: \"Natural Language Processing\" → [\"Natural\", \"Language\", \"Processing\"]\n",
        "\n",
        "- Sentence Tokenization - Splits text into sentences. Example: \"NLP is fascinating. It has various applications.\" → [\"NLP is fascinating.\", \"It has various applications.\"]\n",
        "\n",
        "- Subword Tokenization - Breaks words into smaller units, often useful for languages with complex morphology. Example: \"tokenization\" → [\"to\", \"ken\", \"iza\", \"tion\"]\n",
        "\n",
        "### **Tokenization Techniques**\n",
        "- Rule-Based Tokenization - Discussed on Day-5\n",
        "- Statistical Tokenization - Discussed on Day-5\n",
        "- Tokenization Libraries - Same as Word Tokenization\n",
        "\n",
        "\n",
        "### **Pros and Cons:**\n",
        "\n",
        "- **Pros:**\n",
        "Enables language understanding for machines.\n",
        "Forms the basis for various NLP tasks.\n",
        "Facilitates feature extraction for machine learning models.\n",
        "\n",
        "- **Cons:**\n",
        "Ambiguity in tokenization rules for certain languages or contexts.\n",
        "Challenges with handling slang, abbreviations, and domain-specific terms."
      ],
      "metadata": {
        "id": "kxJlk0DhCDg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Tokenization is a key process in Natural Language Processing. It breaks text into words and sentences.\"\n",
        "\n",
        "# Word Tokenization\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\", word_tokens)"
      ],
      "metadata": {
        "id": "QWEaMnaCD4dH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c0ba85-5063-45fb-c945-35fa03d5edfa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Tokenization', 'is', 'a', 'key', 'process', 'in', 'Natural', 'Language', 'Processing', '.', 'It', 'breaks', 'text', 'into', 'words', 'and', 'sentences', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "print(\"Sentence Tokens:\", sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf2AG_Meym-v",
        "outputId": "522231f5-0891-4d3a-bd9d-2d0b456d7230"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: ['Tokenization is a key process in Natural Language Processing.', 'It breaks text into words and sentences.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subword Tokenization Example\n",
        "word = \"tokenization\"\n",
        "subword_tokens = [word[i:i+2] for i in range(0, len(word), 2)]\n",
        "print(f\"\\nExample: \\\"{word}\\\" → {subword_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkHOsgJgy5aj",
        "outputId": "6083b9c7-df68-4ce0-d2ec-42dcaf8875e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example: \"tokenization\" → ['to', 'ke', 'ni', 'za', 'ti', 'on']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rule-Based Tokenization\n",
        "\"\"\"For rule-based tokenization, we can create custom rules using regular expressions.\"\"\"\n",
        "\n",
        "import re\n",
        "\n",
        "# Dummy Data\n",
        "text = \"Tokenization; splitting text into words, phrases, or other meaningful elements.\"\n",
        "\n",
        "# Rule-Based Tokenization\n",
        "rule_tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "print(\"Rule-Based Tokens:\", rule_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4tuFTeByqqO",
        "outputId": "4fbcf018-42a6-4b86-f854-c72733d16308"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rule-Based Tokens: ['Tokenization', 'splitting', 'text', 'into', 'words', 'phrases', 'or', 'other', 'meaningful', 'elements']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Statistical Tokenization\n",
        "\n",
        "\"\"\"\n",
        "Statistical Tokenization, commonly referred to as Sentence Boundary Detection (SBD),\n",
        "primarily involves using statistical models to determine sentence boundaries in a given text.\n",
        "\"\"\"\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "# Dummy Data\n",
        "text = \"Tokenization is a key process in Natural Language Processing. It breaks text into words and sentences.\"\n",
        "\n",
        "# Statistical Tokenization\n",
        "punkt_tokenizer = PunktSentenceTokenizer()\n",
        "statistical_tokens = punkt_tokenizer.tokenize(text)\n",
        "print(\"Statistical Tokens:\", statistical_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_odNQc7sy9Qx",
        "outputId": "7a83e420-aaf0-48d1-a5b0-edf14f3abc72"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistical Tokens: ['Tokenization is a key process in Natural Language Processing.', 'It breaks text into words and sentences.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of tokenization method depends on the specific requirements of the NLP application and the characteristics of the language being processed.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q0XgKxFSzkMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Stemming**\n",
        "Stemming is a text normalization technique used in Natural Language Processing (NLP) to reduce words to their base or root form. The primary objective of stemming is to map words with similar meanings to a common root, which can simplify the analysis of text data.\n",
        "\n",
        "![Stemming](https://ik.imagekit.io/botpenguin1/assets/website/Stemming_53678d43bc.png)\n",
        "\n",
        "### **How Stemming Works**\n",
        "Stemming involves removing suffixes from words, thereby reducing them to a common base form. This process is based on the intuition that words with the same root convey similar meanings, and treating them as such can aid in various NLP tasks.\n",
        "\n",
        "### **Example of Stemming**\n",
        "\n",
        "- Original Word: \"running\"\n",
        "- Stemmed Word: \"run\"\n",
        "\n",
        "### **Types of Stemming Algorithms**\n",
        "There are various stemming algorithms, and each may have slightly different rules for stemming. One popular stemming algorithm is the Porter Stemmer. **The Porter Stemmer**, designed by Martin Porter, is widely used and implemented in many NLP libraries.\n",
        "\n",
        "**Pros:**\n",
        "- Simplification: Reduces words to a common form, simplifying text analysis.\n",
        "\n",
        "- Dimensionality Reduction: Can help in reducing the dimensionality of feature space in machine learning tasks.\n",
        "\n",
        "\n",
        "**Cons:**\n",
        "- Over-Stemming: May lead to over-stemming, where words are overly reduced, potentially losing some meaning.\n",
        "\n",
        "- Under-Stemming: May under-stem, leaving some words in their original form, leading to less effective normalization."
      ],
      "metadata": {
        "id": "wNQzL4_Kz73t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Create a Porter Stemmer instance\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"flies\", \"happily\", \"better\", \"playing\"]\n",
        "\n",
        "# Stem each word\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Print results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO6kaj6nzDt9",
        "outputId": "c1ed1a88-d19f-4c5b-fc88-409e01d60586"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['running', 'flies', 'happily', 'better', 'playing']\n",
            "Stemmed Words: ['run', 'fli', 'happili', 'better', 'play']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snowball stemming**, also known as the Porter2 stemming algorithm, is an extension of the original Porter stemming algorithm developed by Martin Porter. The Snowball stemming algorithm was designed to improve upon the Porter algorithm by addressing some of its limitations and providing a more aggressive and language-agnostic approach to stemming.\n",
        "\n",
        "While the original Porter stemming algorithm was designed with English in mind, Snowball stemming aims to be more language-agnostic. It provides a framework for creating stemming algorithms for multiple languages.\n",
        "\n",
        "Pros and Cons are same as Porter Stammer."
      ],
      "metadata": {
        "id": "_S2PN0Gy12x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Create a Snowball stemmer instance for English\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"flies\", \"happily\", \"better\", \"playing\"]\n",
        "\n",
        "# Stem each word using Snowball stemmer\n",
        "snowball_stemmed_words = [snowball_stemmer.stem(word) for word in words]\n",
        "\n",
        "# Print results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Snowball Stemmed Words:\", snowball_stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ93w4Xt1k-C",
        "outputId": "b9e2fa6f-c282-42b3-f12d-fabe9c3378ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['running', 'flies', 'happily', 'better', 'playing']\n",
            "Snowball Stemmed Words: ['run', 'fli', 'happili', 'better', 'play']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Choosing Between Porter and Snowball Stemming:**\n",
        "\n",
        "The choice between Porter and Snowball stemming depends on the specific requirements of the task and the characteristics of the text data. Snowball stemming is often preferred for its language-agnostic approach and more aggressive stemming, but it may not always be suitable for every use case. It's recommended to experiment with both stemming algorithms and evaluate their performance based on the specific goals of your NLP application."
      ],
      "metadata": {
        "id": "VBQQeiSK2Eoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Lemmatization**\n",
        "\n",
        "Lemmatization is a text normalization process that involves reducing words to their base or root form, preserving the grammatical meaning of the words. The base form obtained through lemmatization is known as a lemma. Unlike stemming, lemmatization considers the context and part-of-speech (POS) of a word, resulting in more accurate transformations.\n",
        "\n",
        "### **How Lemmatization Works:**\n",
        "- Lemmatization often begins with part-of-speech tagging to determine the grammatical category (noun, verb, adjective, etc.) of each word in a sentence.\n",
        "- Lemmatization involves applying rules specific to each part-of-speech to map words to their base forms.\n",
        "For example, for verbs, lemmatization may involve removing suffixes like \"ing\" or \"ed.\"\n",
        "- Lemmatization may utilize dictionaries or lexicons to look up the base form of words based on their part-of-speech and context.\n",
        "\n",
        "### **Types of Lemmatization:**\n",
        "\n",
        "- **WordNet Lemmatization:**\n",
        "Utilizes the WordNet lexical database to look up lemmas. Effective for English language lemmatization.\n",
        "\n",
        "- **Rule-Based Lemmatization:**\n",
        "Employs predefined rules and linguistic knowledge for lemmatization. Custom rules can be created based on language-specific or domain-specific requirements.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-RmCxw81Ak5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Dummy Data\n",
        "text = \"Lemmatization involves reducing words to their base form, considering the context and part-of-speech.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Lemmatization using WordNet\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# Print Results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDodELQV2Aln",
        "outputId": "175ddea7-11ea-42b0-ff6b-ea3828ddb73c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['Lemmatization', 'involves', 'reducing', 'words', 'to', 'their', 'base', 'form', ',', 'considering', 'the', 'context', 'and', 'part-of-speech', '.']\n",
            "Lemmatized Words: ['Lemmatization', 'involves', 'reducing', 'word', 'to', 'their', 'base', 'form', ',', 'considering', 'the', 'context', 'and', 'part-of-speech', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Part-of-Speech Tagging**\n",
        "It is the process of assigning grammatical categories (tags) to words in a sentence based on their syntactic and semantic roles. Each word is labeled with a specific tag that represents its part of speech, such as noun, verb, adjective, etc. POS tagging is a crucial step in natural language processing as it helps in understanding the grammatical structure of a sentence.\n",
        "\n",
        "### **Working**\n",
        "POS tagging involves analyzing the context of each word in a sentence and assigning the appropriate part-of-speech tag. This is typically done using pre-trained models or rule-based systems that take into account the word's surrounding words and their relationships.\n",
        "\n",
        "### **Types of POS Tags:**\n",
        "\n",
        "- Noun (NN): Represents a person, place, thing, or idea.\n",
        "- Verb (VB): Represents an action or state of being.\n",
        "- Adjective (JJ): Describes a noun or pronoun.\n",
        "Adverb (RB): Describes a verb, adjective, or other adverb.\n",
        "- Pronoun (PRP): Replaces a noun in a sentence.\n",
        "- Preposition (IN): Relates a noun to another word.\n",
        "- Conjunction (CC): Connects words or groups of words.\n",
        "- Interjection (UH): Expresses strong emotion.\n",
        "- Determiner (DT): Specifies a noun as definite or indefinite.\n",
        "- Particle (RP): Small words that don't fit into other categories.\n",
        "\n",
        "**Note:** The POS tags provided by nltk follow the Penn Treebank POS Tag Set, which is widely used in natural language processing."
      ],
      "metadata": {
        "id": "EEF-EmV2OT35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"Part-of-Speech tagging helps in understanding the grammatical structure of a sentence.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Perform POS Tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Print Results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7a_Esl1BLaQ",
        "outputId": "f0350f7c-1da3-4d4a-f4e7-dc9d166d92fe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['Part-of-Speech', 'tagging', 'helps', 'in', 'understanding', 'the', 'grammatical', 'structure', 'of', 'a', 'sentence', '.']\n",
            "POS Tags: [('Part-of-Speech', 'JJ'), ('tagging', 'NN'), ('helps', 'VBZ'), ('in', 'IN'), ('understanding', 'VBG'), ('the', 'DT'), ('grammatical', 'JJ'), ('structure', 'NN'), ('of', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Named Entity Recognition (NER)**\n",
        "It is a Natural Language Processing (NLP) technique that aims to identify and classify entities, such as names of people, organizations, locations, dates, and more, within a given text. It plays a crucial role in extracting structured information from unstructured text data.\n",
        "\n",
        "NER helps extract key information from text, enhancing the understanding of documents. Facilitates efficient searching and retrieval of relevant information. Essential for systems that answer questions by extracting information from texts.\n",
        "\n",
        "### **How NER Works**\n",
        "- Tokenization: Break the text into individual words or tokens.\n",
        "\n",
        "- Part-of-Speech Tagging: Assign a part-of-speech tag to each token.\n",
        "\n",
        "- Named Entity Recognition: Identify and classify tokens into predefined categories (entities).\n",
        "\n",
        "### **Types of Entities:**\n",
        "NER can recognize various types of entities, including but not limited to:\n",
        "\n",
        "- Person: Names of individuals.\n",
        "\n",
        "- Organization: Names of companies, institutions, etc.\n",
        "\n",
        "- Location: Place names, addresses, etc.\n",
        "\n",
        "- Date: References to specific dates.\n",
        "\n",
        "- Time: References to specific times.\n",
        "\n",
        "- Money: Monetary values mentioned in the text.\n",
        "\n",
        "- Percentage: Percentage values mentioned in the text."
      ],
      "metadata": {
        "id": "vLBQ_XvqPQEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input Text\n",
        "text = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak on April 1, 1976, in Cupertino, California.\"\n",
        "\n",
        "# Apply NER\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract NER entities\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "# Print NER entities\n",
        "print(\"Named Entities:\")\n",
        "for entity, label in entities:\n",
        "    print(f\"{label}: {entity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdnwSGqjO1Cg",
        "outputId": "744d3aa7-d244-47cb-aa3b-d903726a2cb3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "ORG: Apple Inc.\n",
            "PERSON: Steve Jobs\n",
            "PERSON: Steve Wozniak\n",
            "DATE: April 1, 1976\n",
            "GPE: Cupertino\n",
            "GPE: California\n",
            "Named Entities:\n",
            "ORG: Apple Inc.\n",
            "PERSON: Steve Jobs\n",
            "PERSON: Steve Wozniak\n",
            "DATE: April 1, 1976\n",
            "GPE: Cupertino\n",
            "GPE: California\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Stopword Removal**\n",
        "Stopword removal is a text pre-processing step where common words that do not contribute significant meaning to the text are eliminated. These words, known as stopwords, include articles, conjunctions, and other frequently occurring words. Removing stopwords helps reduce noise in the data and improves the efficiency of downstream NLP tasks.\n",
        "\n",
        "Common stopwords in English include \"the,\" \"is,\" \"and,\" \"in,\" etc. These words are often necessary for grammar but may not carry specific semantic meaning in certain contexts.\n",
        "\n",
        "### **Working:**\n",
        "\n",
        "- Tokenization:\n",
        "The text is first tokenized into individual words or tokens.\n",
        "\n",
        "- Stopword List: A predefined list of stopwords is used. This list includes words that are considered common and generally uninformative.\n",
        "\n",
        "- Removal: All occurrences of stopwords in the text are removed."
      ],
      "metadata": {
        "id": "Mzh8msCaQcna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the stopwords dataset\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# Dummy Data\n",
        "text = \"Stopword removal is an important step in natural language processing. It helps improve text analysis.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Print Results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Filtered Words (without stopwords):\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLPvnBzUPrxq",
        "outputId": "90eaa614-b0d8-4e67-f8c8-ab40b3a162ef"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['Stopword', 'removal', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '.', 'It', 'helps', 'improve', 'text', 'analysis', '.']\n",
            "Filtered Words (without stopwords): ['Stopword', 'removal', 'important', 'step', 'natural', 'language', 'processing', '.', 'helps', 'improve', 'text', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Data Cleaning and Noise Removal**\n",
        "Data cleaning involves the process of identifying and correcting errors or inconsistencies in the text data. This may include removing irrelevant characters, handling misspelled words, and addressing other issues that can impact the quality of analysis or model performance.\n",
        "\n",
        "**Examples of Data Cleaning:**\n",
        "\n",
        "**Removing Special Characters:**\n",
        "Original Text: \"Hello @World!\"\n",
        "Cleaned Text: \"Hello World\"\n",
        "\n",
        "**Handling Misspelled Words:**\n",
        "Original Text: \"I love programmingg!\"\n",
        "Cleaned Text: \"I love programming!\"\n",
        "\n",
        "**Lowercasing:**\n",
        "Original Text: \"NLP is Exciting!\"\n",
        "Cleaned Text: \"nlp is exciting\""
      ],
      "metadata": {
        "id": "igfeaV9qapM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^A-Za-z ]+', '', text)\n",
        "\n",
        "    # Correct repeated characters (e.g., programmingg to programming)\n",
        "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example\n",
        "original_text = \"Hello @World! I love programmingg! NLP is Exciting!\"\n",
        "cleaned_text = clean_text(original_text)\n",
        "print(\"Original Text:\", original_text)\n",
        "print(\"Cleaned Text:\", cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEKU6HaxQxrW",
        "outputId": "ba5e7268-1bba-4c5c-a8d2-51e62b199090"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Hello @World! I love programmingg! NLP is Exciting!\n",
            "Cleaned Text: helo world i love programing nlp is exciting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Noise Reduction**\n",
        "\n",
        "Noise reduction involves removing irrelevant or distracting information from the text data, enhancing the signal-to-noise ratio. This can include eliminating stop words, irrelevant punctuation, or other elements that do not contribute substantially to the meaning.\n",
        "\n",
        "**Examples of Noise Reduction**:\n",
        "\n",
        "**Removing Stop Words:**\n",
        "\n",
        "**Original Text:**\"The cat is on the mat.\"\n",
        "\n",
        "**Text after Noise Reduction:** \"cat mat\"\n",
        "Eliminating Punctuation:\n",
        "\n",
        "\n",
        "\n",
        "**Original Text:** \"Hello, world!\"\n",
        "\n",
        "**Text after Noise Reduction:** \"Hello world\""
      ],
      "metadata": {
        "id": "oekWzkTXdj9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "def reduce_noise(text):\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "# Example\n",
        "original_text = \"The cat is on the mat. Hello, world!\"\n",
        "text_after_reduction = reduce_noise(original_text)\n",
        "print(\"Original Text:\", original_text)\n",
        "print(\"Text after Noise Reduction:\", text_after_reduction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoIuHMAbdgZK",
        "outputId": "fa7382c5-999a-4d4c-a5cc-8ee12b94674a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The cat is on the mat. Hello, world!\n",
            "Text after Noise Reduction: cat mat . Hello , world !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DAY-7**\n",
        "## **NLP Stage-3**\n",
        "- Word-Embeddings - Word2Vec Explained with Python and Maths\n",
        "\n",
        "**Video Link** -"
      ],
      "metadata": {
        "id": "AGorqld6eJbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V4HwaJzPdzB8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}