{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install -U datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install accelerate\n%pip install peft","metadata":{"execution":{"iopub.status.idle":"2024-02-17T05:49:32.940979Z","shell.execute_reply.started":"2024-02-17T05:49:07.780068Z","shell.execute_reply":"2024-02-17T05:49:32.939661Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.37.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.20.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.8.2-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.8.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# mentioning datatypes for better documentation\nfrom typing import Dict, List\nfrom datasets import Dataset, load_dataset, disable_caching\ndisable_caching() ## disable huggingface cache\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging\n)\nimport torch\nfrom torch.utils.data import Dataset\nfrom IPython.display import Markdown","metadata":{"execution":{"iopub.status.busy":"2024-02-17T05:35:21.040502Z","iopub.execute_input":"2024-02-17T05:35:21.040880Z","iopub.status.idle":"2024-02-17T05:35:29.235909Z","shell.execute_reply.started":"2024-02-17T05:35:21.040847Z","shell.execute_reply":"2024-02-17T05:35:29.234934Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-17 05:35:25.900084: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-17 05:35:25.900156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-17 05:35:25.901764: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Dataset Preparation\ndataset = load_dataset(\"MBZUAI/LaMini-instruction\" , split = 'train') \nsmall_dataset = dataset.select([i for i in range(200)])\nprint(small_dataset)\nprint(small_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-17T05:43:15.469237Z","iopub.execute_input":"2024-02-17T05:43:15.469631Z","iopub.status.idle":"2024-02-17T05:43:16.941060Z","shell.execute_reply.started":"2024-02-17T05:43:15.469600Z","shell.execute_reply":"2024-02-17T05:43:16.940143Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['instruction', 'response', 'instruction_source'],\n    num_rows: 200\n})\n{'instruction': 'List 5 reasons why someone should learn to code', 'response': '1. High demand for coding skills in the job market\\n2. Increased problem-solving and analytical skills\\n3. Ability to develop new products and technologies\\n4. Potentially higher earning potential\\n5. Opportunity to work remotely and/or freelance', 'instruction_source': 'alpaca'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# creating templates\nprompt_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: {instruction}\\n Response:\"\"\"\nanswer_template = \"\"\"{response}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-17T05:43:19.092728Z","iopub.execute_input":"2024-02-17T05:43:19.093459Z","iopub.status.idle":"2024-02-17T05:43:19.097814Z","shell.execute_reply.started":"2024-02-17T05:43:19.093423Z","shell.execute_reply":"2024-02-17T05:43:19.096585Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"instruction = small_dataset[\"instruction\"]\nresponse = small_dataset[\"response\"] \nprint(instruction[0])\nprint(\"-----------------------------\")\nprint(response[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-17T05:43:20.119283Z","iopub.execute_input":"2024-02-17T05:43:20.119626Z","iopub.status.idle":"2024-02-17T05:43:20.126315Z","shell.execute_reply.started":"2024-02-17T05:43:20.119601Z","shell.execute_reply":"2024-02-17T05:43:20.125334Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"List 5 reasons why someone should learn to code\n-----------------------------\n1. High demand for coding skills in the job market\n2. Increased problem-solving and analytical skills\n3. Ability to develop new products and technologies\n4. Potentially higher earning potential\n5. Opportunity to work remotely and/or freelance\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt_template.format(instruction=instruction[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-17T05:43:34.810418Z","iopub.execute_input":"2024-02-17T05:43:34.810779Z","iopub.status.idle":"2024-02-17T05:43:34.817244Z","shell.execute_reply.started":"2024-02-17T05:43:34.810751Z","shell.execute_reply":"2024-02-17T05:43:34.816212Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: List 5 reasons why someone should learn to code\\n Response:'"},"metadata":{}}]},{"cell_type":"code","source":"# creating templates\nprompt_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: {instruction}\\n Response:\"\"\"\nanswer_template = \"\"\"{response}\"\"\"\n\n# creating function to add keys in the dictionary for prompt, answer and whole text\ndef _add_text(rec):\n    instruction = rec[\"instruction\"]\n    response = rec[\"response\"] \n    # check if both exists, else raise error   \n    if not instruction:\n        raise ValueError(f\"Expected an instruction in: {rec}\")\n    if not response:\n        raise ValueError(f\"Expected a response in: {rec}\")\n    rec[\"prompt\"] = prompt_template.format(instruction=instruction)\n    rec[\"answer\"] = answer_template.format(response=response)\n    rec[\"text\"] = rec[\"prompt\"] + rec[\"answer\"]\n    return rec\n\n# running through all samples\nsmall_dataset = small_dataset.map(_add_text)\nprint(small_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-17T05:44:02.721871Z","iopub.execute_input":"2024-02-17T05:44:02.722249Z","iopub.status.idle":"2024-02-17T05:44:02.774333Z","shell.execute_reply.started":"2024-02-17T05:44:02.722219Z","shell.execute_reply":"2024-02-17T05:44:02.773492Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e773a02f32f4d74a1ddb1bc4ce876ec"}},"metadata":{}},{"name":"stdout","text":"{'instruction': 'List 5 reasons why someone should learn to code', 'response': '1. High demand for coding skills in the job market\\n2. Increased problem-solving and analytical skills\\n3. Ability to develop new products and technologies\\n4. Potentially higher earning potential\\n5. Opportunity to work remotely and/or freelance', 'instruction_source': 'alpaca', 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: List 5 reasons why someone should learn to code\\n Response:', 'answer': '1. High demand for coding skills in the job market\\n2. Increased problem-solving and analytical skills\\n3. Ability to develop new products and technologies\\n4. Potentially higher earning potential\\n5. Opportunity to work remotely and/or freelance', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: List 5 reasons why someone should learn to code\\n Response:1. High demand for coding skills in the job market\\n2. Increased problem-solving and analytical skills\\n3. Ability to develop new products and technologies\\n4. Potentially higher earning potential\\n5. Opportunity to work remotely and/or freelance'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# loading the tokenizer for dolly model. The tokenizer converts raw text into tokens\nmodel_id = \"databricks/dolly-v2-3b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\n\"\"\"By setting tokenizer.pad_token = tokenizer.eos_token, the code is instructing the \ntokenizer to use the end-of-sequence token as the padding token.\"\"\"\n\n#loading the model using AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    # use_cache=False,\n    device_map=\"auto\", #\"balanced\",\n    load_in_8bit=False, # No Quantized Model we need\n    torch_dtype=torch.float16\n)\n\n# resizes input token embeddings matrix of the model if new_num_tokens != config.vocab_size.\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-02-17T05:48:47.108180Z","iopub.execute_input":"2024-02-17T05:48:47.108844Z","iopub.status.idle":"2024-02-17T05:48:53.446484Z","shell.execute_reply.started":"2024-02-17T05:48:47.108809Z","shell.execute_reply":"2024-02-17T05:48:53.445527Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Embedding(50280, 2560)"},"metadata":{}}]},{"cell_type":"code","source":"from functools import partial\nimport copy\nfrom transformers import DataCollatorForSeq2Seq\n\nMAX_LENGTH = 256\n\n# Function to generate token embeddings from text part of batch\ndef _preprocess_batch(batch: Dict[str, List]):  \n    model_inputs = tokenizer(batch[\"text\"], max_length=MAX_LENGTH, truncation=True, padding='max_length')    \n    model_inputs[\"labels\"] = copy.deepcopy(model_inputs['input_ids'])\n    return model_inputs\n\n_preprocessing_function = partial(_preprocess_batch)\n\n# apply the preprocessing function to each batch in the dataset\nencoded_small_dataset = small_dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=[\"instruction\", \"response\", \"prompt\", \"answer\"],\n)\nprocessed_dataset = encoded_small_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\n\n# splitting dataset\nsplit_dataset = processed_dataset.train_test_split(test_size=14, seed=0)\nprint(split_dataset)\n\n# takes a list of samples from a Dataset and collate them into a batch, as a dictionary of PyTorch tensors.\ndata_collator = DataCollatorForSeq2Seq(\n        model = model, tokenizer=tokenizer, max_length=MAX_LENGTH, pad_to_multiple_of=8, padding='max_length')","metadata":{"execution":{"iopub.status.busy":"2024-02-17T05:48:53.448541Z","iopub.execute_input":"2024-02-17T05:48:53.448914Z","iopub.status.idle":"2024-02-17T05:48:53.769041Z","shell.execute_reply.started":"2024-02-17T05:48:53.448879Z","shell.execute_reply":"2024-02-17T05:48:53.767880Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f764ba628b44083a74521e6f4ffca6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e67f54480ce64c9e960c3e7eb25b5cb1"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['instruction_source', 'text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 186\n    })\n    test: Dataset({\n        features: ['instruction_source', 'text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 14\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n\nLORA_R = 256 # 512\nLORA_ALPHA = 512 # 1024\nLORA_DROPOUT = 0.05\n# Define LoRA Config\nlora_config = LoraConfig(\n                 r = LORA_R, # the dimension of the low-rank matrices\n                 lora_alpha = LORA_ALPHA, # scaling factor for the weight matrices\n                 lora_dropout = LORA_DROPOUT, # dropout probability of the LoRA layers\n                 bias=\"none\",\n                 task_type=\"CAUSAL_LM\",\n                 target_modules=[\"query_key_value\"],\n)\n\n# Prepare int-8 model for training - utility function that prepares a PyTorch model for int8 quantization training. <https://huggingface.co/docs/peft/task_guides/int8-asr>\nmodel = prepare_model_for_int8_training(model)\n# initialize the model with the LoRA framework\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-02-17T05:49:32.943118Z","iopub.execute_input":"2024-02-17T05:49:32.943484Z","iopub.status.idle":"2024-02-17T05:49:34.155014Z","shell.execute_reply.started":"2024-02-17T05:49:32.943454Z","shell.execute_reply":"2024-02-17T05:49:34.154120Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:143: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 83,886,080 || all params: 2,858,972,160 || trainable%: 2.9341342029717423\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n# define the training arguments first.\nEPOCHS = 3\nLEARNING_RATE = 1e-4  \nMODEL_SAVE_FOLDER_NAME = \"dolly-3b-lora\"\ntraining_args = TrainingArguments(\n                    output_dir=MODEL_SAVE_FOLDER_NAME,\n                    overwrite_output_dir=True,\n                    fp16=True, #converts to float precision 16 using bitsandbytes\n                    per_device_train_batch_size=1,\n                    per_device_eval_batch_size=1,\n                    learning_rate=LEARNING_RATE,\n                    num_train_epochs=EPOCHS,\n                    logging_strategy=\"epoch\",\n                    evaluation_strategy=\"epoch\",\n                    save_strategy=\"epoch\",\n)\n# training the model \ntrainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=split_dataset['train'],\n        eval_dataset=split_dataset[\"test\"],\n        data_collator=data_collator,\n)\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()\n# only saves the incremental 🤗 PEFT weights (adapter_model.bin) that were trained, meaning it is super efficient to store, transfer, and load.\ntrainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n# save the full model and the training arguments\ntrainer.save_model(MODEL_SAVE_FOLDER_NAME)\ntrainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T05:50:12.565865Z","iopub.execute_input":"2024-02-17T05:50:12.566342Z","iopub.status.idle":"2024-02-17T05:54:54.055941Z","shell.execute_reply.started":"2024-02-17T05:50:12.566310Z","shell.execute_reply":"2024-02-17T05:54:54.054156Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240217_055036-nyefwd07</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/simranjeet97/huggingface/runs/nyefwd07' target=\"_blank\">filigreed-dumpling-3</a></strong> to <a href='https://wandb.ai/simranjeet97/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/simranjeet97/huggingface' target=\"_blank\">https://wandb.ai/simranjeet97/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/simranjeet97/huggingface/runs/nyefwd07' target=\"_blank\">https://wandb.ai/simranjeet97/huggingface/runs/nyefwd07</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='373' max='558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [373/558 02:57 < 01:28, 2.09 it/s, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.548100</td>\n      <td>0.562132</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.319000</td>\n      <td>0.585532</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 28\u001b[0m\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     20\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m         data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# only saves the incremental 🤗 PEFT weights (adapter_model.bin) that were trained, meaning it is super efficient to store, transfer, and load.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(MODEL_SAVE_FOLDER_NAME)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1944\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1944\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   1947\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1948\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2300\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2297\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2376\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2375\u001b[0m     staging_output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2376\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstaging_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   2379\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   2380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(staging_output_dir)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2882\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   2879\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   2881\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2882\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2884\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   2885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2942\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   2940\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer.model is not a `PreTrainedModel`, only saving its state dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_safetensors:\n\u001b[0;32m-> 2942\u001b[0m     \u001b[43msafetensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAFE_WEIGHTS_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m   2944\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2946\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/safetensors/torch.py:281\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    251\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    252\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    253\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m ):\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mSafetensorError\u001b[0m: Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })"],"ename":"SafetensorError","evalue":"Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })","output_type":"error"}]},{"cell_type":"code","source":"# Function to format the response and filter out the instruction from the response.\ndef postprocess(response):\n    messages = response.split(\"Response:\")\n    if not messages:\n        raise ValueError(\"Invalid template for prompt. The template should include the term 'Response:'\")\n    return \"\".join(messages[1:])\n# Prompt for prediction\ninference_prompt = \"List 5 reasons why someone should learn to cook\"\n# Inference pipeline with the fine-tuned model\ninf_pipeline =  pipeline('text-generation', model=trainer.model, tokenizer=tokenizer, max_length=256, trust_remote_code=True)\n# Format the prompt using the `prompt_template` and generate response \nresponse = inf_pipeline(prompt_template.format(instruction=inference_prompt))[0]['generated_text']\n# postprocess the response\nformatted_response = postprocess(response)\nformatted_response","metadata":{"execution":{"iopub.status.busy":"2024-02-17T05:54:54.057548Z","iopub.status.idle":"2024-02-17T05:54:54.057957Z","shell.execute_reply.started":"2024-02-17T05:54:54.057718Z","shell.execute_reply":"2024-02-17T05:54:54.057733Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/service/streams.py\", line 48, in run\n    self._target(**self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 174, in wandb_internal\n    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\nMessage: 'Thread WriterThread:'\nArguments: ()\nThread WriterThread:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 380, in _process\n    self._wm.write(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/writer.py\", line 154, in write\n    write_handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/writer.py\", line 135, in _write\n    self._write_record(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/writer.py\", line 109, in _write_record\n    ret = self._ds.write(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/datastore.py\", line 291, in write\n    ret = self._write_data(s)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/datastore.py\", line 247, in _write_data\n    self._write_record(s)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/datastore.py\", line 226, in _write_record\n    self._fp.write(s)\nOSError: [Errno 28] No space left on device\nwandb: ERROR Internal wandb error: file data was not synced\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}