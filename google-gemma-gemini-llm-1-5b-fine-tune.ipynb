{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-21T09:30:09.162054Z","iopub.execute_input":"2024-03-21T09:30:09.162669Z","iopub.status.idle":"2024-03-21T09:30:10.862015Z","shell.execute_reply.started":"2024-03-21T09:30:09.162630Z","shell.execute_reply":"2024-03-21T09:30:10.861062Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"![](https://pbs.twimg.com/media/GG3SidNWYAAljb1?format=jpg&name=4096x4096)","metadata":{}},{"cell_type":"markdown","source":"# To Know about Google Gemini Architecture and Parameters, Watch this Video\n## Link - https://youtu.be/vabQP3UkWDc","metadata":{}},{"cell_type":"markdown","source":"### Key Features of Gemini\n- Enhanced Contextual Understanding: Gemini is the first model to outperform human experts on MMLU (Massive Multitask Language Understanding), one of the most popular methods to test the knowledge and problem-solving abilities of AI models.\n- Multimodality: Gemini is built from the ground up for multimodality — reasoning seamlessly across text, images, video, audio, and code.\n- Anything to anything: Gemini is natively multimodal, which gives you the potential to transform any type of input into any type of output.\n- Customizability: Users can fine-tune Gemini for specific tasks or industries.\n","metadata":{}},{"cell_type":"markdown","source":"# NoteBook - Google Gemini Gemma Models Tutorials - 1\n\n### Exmaple Code to Run Google Gemma Models","metadata":{}},{"cell_type":"code","source":"# Setup the environment\n!pip install --upgrade huggingface_hub","metadata":{"execution":{"iopub.status.idle":"2024-03-21T09:37:12.423748Z","shell.execute_reply.started":"2024-03-21T09:36:59.020086Z","shell.execute_reply":"2024-03-21T09:37:12.422654Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Installing collected packages: huggingface_hub\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.20.3\n    Uninstalling huggingface-hub-0.20.3:\n      Successfully uninstalled huggingface-hub-0.20.3\nSuccessfully installed huggingface_hub-0.21.4\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers -U\n!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install trl\n!pip install git+https://github.com/huggingface/peft.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login(write_permission=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T09:42:58.816079Z","iopub.execute_input":"2024-03-21T09:42:58.816714Z","iopub.status.idle":"2024-03-21T09:42:58.847169Z","shell.execute_reply.started":"2024-03-21T09:42:58.816682Z","shell.execute_reply":"2024-03-21T09:42:58.846311Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d2ff65ac29847218c25cfb10ca8ced4"}},"metadata":{}}]},{"cell_type":"markdown","source":"The gemma models will be loaded as a pytorch model, so we can use torch to specify datatypes and other aspects of our model when we load it in. By default they will be loaded as torch.float32, meaning each weight will be made up of 32 bits of information, for our 7b model this means it will take about 32gb of memory needed to load this model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\n\"\"\"Gemma 2b and 7b has a context window of 8192 tokens\n\n\n\"\"\"\nmodel_checkpoint = \"google/gemma-2b-it\"\n\n# Load the model\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_checkpoint, device_map=\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:44:52.525773Z","iopub.execute_input":"2024-03-20T12:44:52.526699Z","iopub.status.idle":"2024-03-20T12:45:15.684740Z","shell.execute_reply.started":"2024-03-20T12:44:52.526646Z","shell.execute_reply":"2024-03-20T12:45:15.683492Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"699643e3e84449219fccef8679756694"}},"metadata":{}}]},{"cell_type":"code","source":"print(model)\nprint(model.dtype)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:45:15.686660Z","iopub.execute_input":"2024-03-20T12:45:15.687057Z","iopub.status.idle":"2024-03-20T12:45:15.698816Z","shell.execute_reply.started":"2024-03-20T12:45:15.687017Z","shell.execute_reply":"2024-03-20T12:45:15.697582Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)\ntorch.float32\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"7 Wonders of World?\"\n\ntoken_inputs = tokenizer(prompt, return_tensors=\"pt\")\n\ntoken_outputs = model.generate(input_ids=token_inputs['input_ids'], max_new_tokens=500)\n# since this is exploratory we'll decode special tokens as well. for this prompt\nnew_tokens = token_outputs[0][token_inputs['input_ids'].shape[-1]:]\n# it will be the beginning <bos> token and the ending <eos> tokens.\ndecoded_output = tokenizer.decode(new_tokens, skip_special_tokens=False)\ndecoded_output","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:45:15.700247Z","iopub.execute_input":"2024-03-20T12:45:15.700651Z","iopub.status.idle":"2024-03-20T12:52:05.262933Z","shell.execute_reply.started":"2024-03-20T12:45:15.700616Z","shell.execute_reply":"2024-03-20T12:52:05.261700Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"\"\\n\\nSure, here's a list of the 77 Wonders of the World:\\n\\n1. Great Pyramid of Giza\\n2. The Great Sphinx\\n3. Petra's Treasury\\n4. The Hanging Gardens of Babylon\\n5. The Great Wall of China\\n6. The Colosseum in Rome\\n7. The Statue of Zeus at Olympia\\n8. The Hanging Gardens of Babylon\\n9. The Great Pyramid of Giza\\n10. The Valley of the Kings in Egypt\\n11. The Temple of Artemis at Ephesus\\n12. The Hanging Gardens of Babylon\\n13. The Great Sphinx\\n14. The Valley of the Kings in Egypt\\n15. The Statue of Zeus at Olympia\\n16. The Hanging Gardens of Babylon\\n17. The Great Pyramid of Giza\\n18. The Valley of the Kings in Egypt\\n19. The Temple of Artemis at Ephesus\\n20. The Hanging Gardens of Babylon\\n21. The Great Pyramid of Giza\\n22. The Valley of the Kings in Egypt\\n23. The Temple of Artemis at Ephesus\\n24. The Hanging Gardens of Babylon\\n25. The Great Pyramid of Giza\\n26. The Valley of the Kings in Egypt\\n27. The Temple of Artemis at Ephesus\\n28. The Hanging Gardens of Babylon\\n29. The Great Pyramid of Giza\\n30. The Valley of the Kings in Egypt\\n31. The Temple of Artemis at Ephesus\\n32. The Hanging Gardens of Babylon\\n33. The Great Pyramid of Giza\\n34. The Valley of the Kings in Egypt\\n35. The Temple of Artemis at Ephesus\\n36. The Hanging Gardens of Babylon\\n37. The Great Pyramid of Giza\\n38. The Valley of the Kings in Egypt\\n39. The Temple of Artemis at Ephesus\\n40. The Hanging Gardens of Babylon\\n41. The Great Pyramid of Giza\\n42. The Valley of the Kings in Egypt\\n43. The Temple of Artemis at Ephesus\\n44. The Hanging Gardens of Babylon\\n45. The Great Pyramid of Giza\\n46. The Valley of the Kings in Egypt\\n47. The Temple of Artemis at Ephesus\\n48. The Hanging Gardens of Babylon\\n49. The Great Pyramid of Giza\\n50. The Valley of the Kings in Egypt\\n51\""},"metadata":{}}]},{"cell_type":"code","source":"print(decoded_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine Tuning with LORA","metadata":{}},{"cell_type":"code","source":"!pip install -U datasets\n!pip install trl\n!pip install git+https://github.com/huggingface/peft.git\n# Restar the Notebook After this and Then Load Tokenizer and Model and Then Load the Dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import load_dataset\nfrom transformers import Trainer, TrainingArguments\n\n# Load the pre-trained model and tokenizer\nmodel_id = \"google/gemma-2b\"\nlora_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=lora_config)\n\n# Define PEFT configuration\npeft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    r=4,\n    lora_alpha=16,\n    lora_dropout=0.01,\n)\n\n# Attach trainable adapters to the quantized model\npeft_model = get_peft_model(model, peft_config)\n\n# Load the dataset\ndata = load_dataset(\"Abirate/english_quotes\")\ndata = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n\nimport transformers\nfrom trl import SFTTrainer\n\ndef formatting_func(example):\n    text = f\"Quote: {data['train']['quote'][0]}\\nAuthor: {data['train']['author'][0]}\"\n    return [text]","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:34:50.666777Z","iopub.execute_input":"2024-03-20T12:34:50.667164Z","iopub.status.idle":"2024-03-20T12:35:08.599773Z","shell.execute_reply.started":"2024-03-20T12:34:50.667136Z","shell.execute_reply":"2024-03-20T12:35:08.598685Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-20 12:34:57.466464: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-20 12:34:57.466528: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-20 12:34:57.468303: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\nGemma's activation function should be approximate GeLU and not exact GeLU.\nChanging the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c994d70753d4460fafcf152b2b9eb99e"}},"metadata":{}}]},{"cell_type":"code","source":"# Define TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"./fine-tuned_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=1,  # Reduce batch size\n    gradient_accumulation_steps=4,  # Accumulate gradients\n    learning_rate=2e-4,\n    fp16=True,  # Enable mixed-precision training\n    logging_steps=1,\n    optim=\"paged_adamw_8bit\"\n)\n\n# Create SFTTrainer\ntrainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=data[\"train\"],\n    args=training_args,\n    peft_config=peft_config,\n    formatting_func=formatting_func,\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:35:17.699372Z","iopub.execute_input":"2024-03-20T12:35:17.699773Z","iopub.status.idle":"2024-03-20T12:36:00.299604Z","shell.execute_reply.started":"2024-03-20T12:35:17.699742Z","shell.execute_reply":"2024-03-20T12:36:00.298647Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimranjeetsingh1497\u001b[0m (\u001b[33msimranjeet97\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240320_123522-pxa66hr8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/simranjeet97/huggingface/runs/pxa66hr8' target=\"_blank\">clean-rain-9</a></strong> to <a href='https://wandb.ai/simranjeet97/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/simranjeet97/huggingface' target=\"_blank\">https://wandb.ai/simranjeet97/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/simranjeet97/huggingface/runs/pxa66hr8' target=\"_blank\">https://wandb.ai/simranjeet97/huggingface/runs/pxa66hr8</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:03, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.032200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.003200</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.982800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3, training_loss=1.0060708324114482, metrics={'train_runtime': 40.1608, 'train_samples_per_second': 0.224, 'train_steps_per_second': 0.075, 'total_flos': 1926839549952.0, 'train_loss': 1.0060708324114482, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"peft_model.save_pretrained(\"gemma_2b_quotes_Fine_tuned\")\ntokenizer.save_pretrained(\"gemma_2b_quotes_Fine_tuned_tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:37:18.460501Z","iopub.execute_input":"2024-03-20T12:37:18.461363Z","iopub.status.idle":"2024-03-20T12:37:19.224261Z","shell.execute_reply.started":"2024-03-20T12:37:18.461325Z","shell.execute_reply":"2024-03-20T12:37:19.222988Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"('gemma_2b_quotes_Fine_tuned_tokenizer/tokenizer_config.json',\n 'gemma_2b_quotes_Fine_tuned_tokenizer/special_tokens_map.json',\n 'gemma_2b_quotes_Fine_tuned_tokenizer/tokenizer.model',\n 'gemma_2b_quotes_Fine_tuned_tokenizer/added_tokens.json',\n 'gemma_2b_quotes_Fine_tuned_tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/gemma_2b_quotes_Fine_tuned_tokenizer\")\nquantized_model = AutoModelForCausalLM.from_pretrained(\"/kaggle/working/gemma_2b_quotes_Fine_tuned\")","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:38:34.611924Z","iopub.execute_input":"2024-03-20T12:38:34.612353Z","iopub.status.idle":"2024-03-20T12:38:40.710075Z","shell.execute_reply.started":"2024-03-20T12:38:34.612318Z","shell.execute_reply":"2024-03-20T12:38:40.708852Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4aef5e1e5f47869027f916d23fab5a"}},"metadata":{}}]},{"cell_type":"code","source":"text = \"Quote: Life is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\noutputs = quantized_model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:40:06.066549Z","iopub.execute_input":"2024-03-20T12:40:06.067466Z","iopub.status.idle":"2024-03-20T12:40:45.315390Z","shell.execute_reply.started":"2024-03-20T12:40:06.067429Z","shell.execute_reply":"2024-03-20T12:40:45.313655Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Quote: Life is a journey, not a destination.\n\nI am a wife, mother, and a professional photographer. I am a lover of all things creative, and I am a dreamer. I am a dreamer of dreams, and I am a dreamer of possibilities.\n","output_type":"stream"}]},{"cell_type":"code","source":"total_params = sum(p.numel() for p in quantized_model.parameters())\nprint(f\"Total parameters: {total_params}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:41:34.931581Z","iopub.execute_input":"2024-03-20T12:41:34.932501Z","iopub.status.idle":"2024-03-20T12:41:34.944507Z","shell.execute_reply.started":"2024-03-20T12:41:34.932466Z","shell.execute_reply":"2024-03-20T12:41:34.943393Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Total parameters: 2506633216\n","output_type":"stream"}]}]}