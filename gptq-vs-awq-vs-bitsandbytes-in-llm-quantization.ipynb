{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Quantization \nA Large Language Model is represented by a bunch of weights and activations. These values are generally represented by the usual 32-bit floating point (float32) datatype.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KLPKaFB9_f6jKcyl.png)\n\nAs you might expect, if we choose a lower bit size, then the model becomes less accurate but it also needs to represent fewer values, thereby decreasing its size and memory requirements.\n\nThere are three popular quantization methods for LLMs, we will discuss briefly about each of them, then we will look into how to use each of the techniques in code.\n\n- GPTQ\n- AWQ\n- Bitsandbytes NF4\n\n## GPTQ Quantization\nGPTQ is a post-training quantization (PTQ) method for quantization that focuses primarily on GPU inference and performance.\n\nPost-training quantization (PTQ) is the sort of quantization, wherein the weights of an already skilled model are transformed to a lower precision without any retraining. This is a trustworthy and clean-to-put-into-effect method\n\nIt is based on one-shot weight quantization method based on approximate second-order information, that is both highly- accurate and highly-efficient.\n\nThe idea behind the method is that it will try to compress all weights to a 4-bit quantization by minimizing the mean squared error to that weight between the original layer and the new quantized layer.","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install transformers optimum accelerate auto-gptq bitsandbytes accelerate","metadata":{"execution":{"iopub.status.busy":"2024-04-05T10:55:14.045078Z","iopub.execute_input":"2024-04-05T10:55:14.045365Z","iopub.status.idle":"2024-04-05T10:55:35.771394Z","shell.execute_reply.started":"2024-04-05T10:55:14.045342Z","shell.execute_reply":"2024-04-05T10:55:35.770182Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\nraw_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")","metadata":{"execution":{"iopub.status.busy":"2024-04-05T11:02:17.754343Z","iopub.execute_input":"2024-04-05T11:02:17.755055Z","iopub.status.idle":"2024-04-05T11:02:29.792128Z","shell.execute_reply.started":"2024-04-05T11:02:17.755017Z","shell.execute_reply":"2024-04-05T11:02:29.791048Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c7d249e26db41c08208c2beda15700b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f911cbe934cd4998a0aa3cc2a17364f4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27d7ae7bbddd429ca4aed0d84a7f8bef"}},"metadata":{}}]},{"cell_type":"code","source":"total_params = sum(p.numel() for p in raw_model.parameters())\nprint(\"Total Parameters:\", total_params)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T11:02:29.793863Z","iopub.execute_input":"2024-04-05T11:02:29.794321Z","iopub.status.idle":"2024-04-05T11:02:29.800763Z","shell.execute_reply.started":"2024-04-05T11:02:29.794294Z","shell.execute_reply":"2024-04-05T11:02:29.799648Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Total Parameters: 125239296\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n\nmodel_id = \"facebook/opt-125m\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nquantization_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T11:02:29.802107Z","iopub.execute_input":"2024-04-05T11:02:29.802414Z","iopub.status.idle":"2024-04-05T11:06:03.509350Z","shell.execute_reply.started":"2024-04-05T11:02:29.802390Z","shell.execute_reply":"2024-04-05T11:06:03.508209Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5b673be93c64335be0adcd800f512da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"248f19e1044d48e6853b3836d78e49d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"744fdaafa6264b1184d2dfd46d99fa65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20aaeda0ed3e491181a25f3f650b2ca0"}},"metadata":{}},{"name":"stderr","text":"2024-04-05 11:02:32.593599: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-05 11:02:32.593720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-05 11:02:32.733244: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Downloading and preparing dataset json/allenai--c4 to /root/.cache/huggingface/datasets/json/allenai--c4-ec45c889631c3c39/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bcd7b87e17f48a3b2f3319d754944be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"903e8362d3e7476db2bb4f3da5b55bbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eb9fa34b0424dd8ab383ebe7616a82a"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/allenai--c4-ec45c889631c3c39/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Quantizing model.decoder.layers blocks :   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07dcea019705467983f5993bfec82111"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(\"Total Parameters:\", total_params)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T11:06:03.511945Z","iopub.execute_input":"2024-04-05T11:06:03.512594Z","iopub.status.idle":"2024-04-05T11:06:03.518716Z","shell.execute_reply.started":"2024-04-05T11:06:03.512565Z","shell.execute_reply":"2024-04-05T11:06:03.517822Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Total Parameters: 40221696\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## AWQ Quantization\nAWQ (Activation-aware Weight Quantization) which is a quantization method similar to GPTQ. There are several differences between AWQ and GPTQ as methods but the most important one is that AWQ assumes that not all weights are equally important for an LLM’s performance.\n\nIn other words, a small fraction of weights will be skipped during quantization, which helps with the quantization loss.\n\nAs a result, their paper mentions a significant speed-up compared to GPTQ whilst keeping similar, and sometimes even better, performance.\n\n### Quantize Model to AWQ\nAWQ performs zero point quantization down to a precision of 4-bit integers.\n\n### AutoAWQ integration with Transformers\n\nNote:\n- Some models like Falcon is only compatible with group size 64.\n- To use Marlin, you must specify zero point as False and version as Marlin.\n","metadata":{}},{"cell_type":"code","source":"!pip install autoawq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = 'mistralai/Mistral-7B-Instruct-v0.2'\n# quant_path = 'mistral-instruct-v0.2-fbc-awq'\nquant_config = { \"zero_point\": False, \"q_group_size\": 64, \"w_bit\": 4, \"version\": \"GEMM\" }\n\nraw_model = AutoModelForCausalLM.from_pretrained(model_path)\n\n# Load model\nq_model = AutoAWQForCausalLM.from_pretrained(\n    model_path, **{\"low_cpu_mem_usage\": False, \"use_cache\": False}\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Quantize\nq_model.quantize(tokenizer, quant_config=quant_config)\n\n# # Save quantized model\n# model.save_quantized(quant_path)\n# tokenizer.save_pretrained(quant_path)\n\ntotal_params_raw = sum(p.numel() for p in raw_model.parameters())\ntotal_params_q = sum(p.numel() for p in q_model.parameters())\nprint(\"Total Parameters:\", total_params_raw)\nprint(\"Total Parameters:\", total_params_q)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bitsandbytes NF4\nbitsandbytes is the easiest way for quantizing a model to 8 and 4-bit\n\nIt works in three steps:\n\n- Normalization: The weights of the model are normalized so that we expect the weights to fall within a certain range. This allows for more efficient representation of more common values.\n- Quantization: The weights are quantized to 4-bit. In NF4, the quantization levels are evenly spaced with respect to the normalized weights, thereby efficiently representing the original 32-bit weights.\n- Dequantization: Although the weights are stored in 4-bit, they are dequantized during computation which gives a performance boost during inference.\nIt represents the weights with 4-bit quantization but does the inference in 16-bit.\n\nThis is a wonderful technique but it seems rather wasteful to have to apply them every time we have to load the model.\n\n### Quantise model with Bitsandbytes\nUsing this quantisation is straightforward with HuggingFace. we just need to define a configuration for the quantization with Bitsandbytes:","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\nfrom torch import bfloat16\n\n# Our 4-bit configuration to load the LLM with less GPU memory\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # 4-bit quantization\n    bnb_4bit_quant_type='nf4',  # Normalized float 4\n    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n    bnb_4bit_compute_dtype=bfloat16  # Computation type\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T11:06:03.520011Z","iopub.execute_input":"2024-04-05T11:06:03.520746Z","iopub.status.idle":"2024-04-05T11:06:03.530923Z","shell.execute_reply.started":"2024-04-05T11:06:03.520700Z","shell.execute_reply":"2024-04-05T11:06:03.529906Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"This configuration allows us to specify which quantization levels we are going for. Generally, we want to represent the weights with 4-bit quantization but do the inference in 16-bit.\n\nLoading the model in a pipeline is then straightforward:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\n# Zephyr with BitsAndBytes Configuration\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"HuggingFaceH4/zephyr-7b-alpha\",\n    quantization_config=bnb_config,\n    device_map='auto',\n)\n\n# Create a pipeline\npipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')\n\nprompt = '''\n<|system|> \\\nYou are a friendly chatbot who always responds in the style of a pirate.\\\n</s>\\\n<|user|>\\\nWrite a tweet on future of AI. </s>\\\n<|assistant|>\n'''\n\noutputs = pipe(\n    prompt, \n    max_new_tokens=256, \n    do_sample=True, \n    temperature=0.7, \n    top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-04-05T11:06:03.532215Z","iopub.execute_input":"2024-04-05T11:06:03.532481Z","iopub.status.idle":"2024-04-05T11:11:47.874135Z","shell.execute_reply.started":"2024-04-05T11:06:03.532458Z","shell.execute_reply":"2024-04-05T11:11:47.873134Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72330e28f5f64648bce2814ce43de682"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4612b1da375f46759e04fa5ec6f4fb26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c1ba9e52fdd4ce19c3a51ffe4d0492e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf87c701e89d41f4a64c3925c79e4ede"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fe640a537864794b52242747ef64cbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/628 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6518cc937fcf4ee3916683bccd90b976"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c59c9fb83a7a471391f89047987c6752"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65fb80e5b3fb42dea395ae6159e8c558"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bbd3cfbd790458bb62dcbb61e3a6b74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a82438fc6ef54013ac72854d8021c31d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cbf5067e6994b6c88fae3fb4bbad729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e25c40ea33e48e08cd2c3666b8ee848"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15288ed2022443e098974b25917ced54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7b5bede0888479aa916d3541b9dcdbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6530152957c4ea3ab8a3e2249976a31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99860347d042401684ff8c15a8a38108"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfbbb804736a48c89f4a82bff1c7c569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99a1d6ff057d45e2ab1f30cc3ed7a676"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n<|system|> You are a friendly chatbot who always responds in the style of a pirate.</s><|user|>Write a tweet on future of AI. </s><|assistant|>\n\"Ahoy there, matey! The future of AI be lookin' mighty fine, with advancements in machine learning, natural language processing, and neural networks. Brace yerself for a new era of intelligent automation and augmented intelligence!\" #AI #FutureofAI #IntelligentAutomation #AugmentedIntelligence #MachineLearning #NaturalLanguageProcessing\n","output_type":"stream"}]},{"cell_type":"code","source":"raw_model = AutoModelForCausalLM.from_pretrained(\n    \"HuggingFaceH4/zephyr-7b-alpha\",\n    device_map='auto',\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T11:11:47.876573Z","iopub.execute_input":"2024-04-05T11:11:47.877229Z","iopub.status.idle":"2024-04-05T11:12:50.638684Z","shell.execute_reply.started":"2024-04-05T11:11:47.877190Z","shell.execute_reply":"2024-04-05T11:12:50.637747Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0e92c5a0fd04f0bb3a44031c002da39"}},"metadata":{}}]},{"cell_type":"code","source":"total_params_raw = sum(p.numel() for p in raw_model.parameters())\ntotal_params_q = sum(p.numel() for p in model.parameters())\nprint(\"Total Parameters:\", total_params_raw)\nprint(\"Total Parameters:\", total_params_q)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T11:12:50.639933Z","iopub.execute_input":"2024-04-05T11:12:50.640250Z","iopub.status.idle":"2024-04-05T11:12:50.651242Z","shell.execute_reply.started":"2024-04-05T11:12:50.640223Z","shell.execute_reply":"2024-04-05T11:12:50.650100Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Total Parameters: 7241732096\nTotal Parameters: 3752071168\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Bitsandbytes vs GPTQ vs AWQ\nA quick camparition between Bitsandbytes, GPTQ and AWQ quantization, so you can choose which methods to use according to your use case.\n\n## Bitandbytes\n- This method quantise the model using HF weights, so very easy to implement\n- Slower than other quantisation methods as well as 16-bit LLM model.\n- This is a wonderful technique but it seems rather wasteful to have to apply them every time we have to load the model.\n- Take longer time to load the models weights\n\n## GPTQ\n- Much Faster as compared to Bitandbytes\n- New model architectures are promptly supported in AutoGPTQ\n\n### Challenges\n- Need to quantise the model weights to GPTQ weights beforehand to use it in production.\n- High computation to quantise the model\n- Around ~ 16GB GPU memory required to quantise 7B parameter model\n\n## AWQ\n- Paper mentions a significant speed-up compared to GPTQ whilst keeping similar, and sometimes even better, performance\n\n### Limitations\n- Newer architectures like Gemma or DeciLM doesn’t support in AWQ quantization yet.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}